{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhJ7px_V4u_B"
      },
      "outputs": [],
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "# full file\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"Put the propoer file ID here"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('HLA.recode.vcf')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"Put the propoer file ID here\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('integrated_call_samples.20130502.ALL.ped')  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liJJGzQp4qzO",
        "outputId": "754ddce3-9282-4284-e244-84fd8c55e79c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.16.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.21.5)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (0.11.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-addons\n",
        "!pip install pyyaml h5py\n",
        "!pip install toolz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWzi3Z3L5RO2",
        "outputId": "7cce7f07-43a8-4c46-e1f6-bde70e72692e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjGOO5PdFPf7"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odmhCqSVFPf8",
        "outputId": "4943de94-3adc-4070-cbe0-ab5586aa6707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# os.environ[\"MODIN_CPUS\"] = \"8\"\n",
        "# from distributed import Client\n",
        "# client = Client()\n",
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow_addons as tfa\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import constraints\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.applications import efficientnet as efn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "# import allel\n",
        "from scipy.spatial.distance import squareform\n",
        "%matplotlib inline   \n",
        "from toolz import interleave\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
        "from sklearn.model_selection import KFold,StratifiedKFold\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "# tf.config.experimental_connect_to_cluster(resolver)\n",
        "# # This is the TPU initialization code that has to be at the beginning.\n",
        "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "# strategy = tf.distribute.TPUStrategy(resolver)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLd26RspFhaS"
      },
      "source": [
        "## Hardware Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLd7mAFgFUnR",
        "outputId": "bed06ea4-bc09-454e-e123-4772bd005598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  grpc://10.123.156.82:8470\n",
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.123.156.82:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.123.156.82:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N_REPLICAS: 8\n"
          ]
        }
      ],
      "source": [
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
        "    print('Running on TPU ', TPU.master())\n",
        "except ValueError:\n",
        "    print('Running on GPU')\n",
        "    TPU = None\n",
        "\n",
        "if TPU:\n",
        "    tf.config.experimental_connect_to_cluster(TPU)\n",
        "    tf.tpu.experimental.initialize_tpu_system(TPU)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(TPU)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "\n",
        "N_REPLICAS = strategy.num_replicas_in_sync\n",
        "# Number of computing cores, is 8 for a TPU V3-8\n",
        "print(f'N_REPLICAS: {N_REPLICAS}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A77GFE3xFPf8"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3zy8i_8FPf_"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "vcf_file = 'HLA.recode.vcf'\n",
        "\n",
        "# load genotype\n",
        "genotypes = pd.read_csv(vcf_file, sep='\\t', index_col=0)\n",
        "\n",
        "headers = genotypes.columns[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "H2LdnKzXnW0P",
        "outputId": "5baea23b-6039-4209-ee44-2c6a5a589cac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        6_33500639 6_33500853 6_33500930 6_33501330 6_33501648 6_33501844  \\\n",
              "HG00096        0|0        0|0        0|0        0|0        0|0        0|0   \n",
              "HG00097        0|0        1|0        1|0        0|0        1|0        0|0   \n",
              "HG00099        0|0        0|1        0|0        0|0        0|0        0|0   \n",
              "HG00100        0|0        0|1        0|1        0|0        0|1        0|0   \n",
              "HG00101        0|0        1|1        1|0        0|0        1|0        0|0   \n",
              "\n",
              "        6_33503018 6_33503323 6_33503878 6_33504462  ... 6_36596648  \\\n",
              "HG00096        0|0        0|0        0|0        0|0  ...        0|0   \n",
              "HG00097        0|0        0|0        0|0        1|0  ...        0|1   \n",
              "HG00099        0|0        0|0        0|0        0|0  ...        1|1   \n",
              "HG00100        0|0        0|0        0|0        0|1  ...        0|1   \n",
              "HG00101        0|0        0|0        0|0        1|0  ...        1|0   \n",
              "\n",
              "        6_36596784 6_36597695 6_36597907 6_36598178 6_36598209 6_36598837  \\\n",
              "HG00096        0|0        0|0        0|0        0|0        0|0        0|0   \n",
              "HG00097        0|1        0|1        0|1        0|0        0|1        0|1   \n",
              "HG00099        1|1        1|1        1|1        0|0        1|1        1|1   \n",
              "HG00100        0|1        0|1        0|1        0|0        0|1        0|1   \n",
              "HG00101        1|0        1|0        1|0        0|0        1|0        1|0   \n",
              "\n",
              "        6_36599010 6_36599596 6_36599812  \n",
              "HG00096        0|0        0|0        0|0  \n",
              "HG00097        0|1        0|1        0|0  \n",
              "HG00099        1|1        1|1        0|0  \n",
              "HG00100        0|1        0|1        0|0  \n",
              "HG00101        1|0        1|0        0|0  \n",
              "\n",
              "[5 rows x 7161 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2cb94b67-679e-4433-98ff-6513be58ad59\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>6_33500639</th>\n",
              "      <th>6_33500853</th>\n",
              "      <th>6_33500930</th>\n",
              "      <th>6_33501330</th>\n",
              "      <th>6_33501648</th>\n",
              "      <th>6_33501844</th>\n",
              "      <th>6_33503018</th>\n",
              "      <th>6_33503323</th>\n",
              "      <th>6_33503878</th>\n",
              "      <th>6_33504462</th>\n",
              "      <th>...</th>\n",
              "      <th>6_36596648</th>\n",
              "      <th>6_36596784</th>\n",
              "      <th>6_36597695</th>\n",
              "      <th>6_36597907</th>\n",
              "      <th>6_36598178</th>\n",
              "      <th>6_36598209</th>\n",
              "      <th>6_36598837</th>\n",
              "      <th>6_36599010</th>\n",
              "      <th>6_36599596</th>\n",
              "      <th>6_36599812</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>HG00096</th>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>...</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HG00097</th>\n",
              "      <td>0|0</td>\n",
              "      <td>1|0</td>\n",
              "      <td>1|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>1|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>1|0</td>\n",
              "      <td>...</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HG00099</th>\n",
              "      <td>0|0</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>...</td>\n",
              "      <td>1|1</td>\n",
              "      <td>1|1</td>\n",
              "      <td>1|1</td>\n",
              "      <td>1|1</td>\n",
              "      <td>0|0</td>\n",
              "      <td>1|1</td>\n",
              "      <td>1|1</td>\n",
              "      <td>1|1</td>\n",
              "      <td>1|1</td>\n",
              "      <td>0|0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HG00100</th>\n",
              "      <td>0|0</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|1</td>\n",
              "      <td>...</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|1</td>\n",
              "      <td>0|0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HG00101</th>\n",
              "      <td>0|0</td>\n",
              "      <td>1|1</td>\n",
              "      <td>1|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>1|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>1|0</td>\n",
              "      <td>...</td>\n",
              "      <td>1|0</td>\n",
              "      <td>1|0</td>\n",
              "      <td>1|0</td>\n",
              "      <td>1|0</td>\n",
              "      <td>0|0</td>\n",
              "      <td>1|0</td>\n",
              "      <td>1|0</td>\n",
              "      <td>1|0</td>\n",
              "      <td>1|0</td>\n",
              "      <td>0|0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 7161 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2cb94b67-679e-4433-98ff-6513be58ad59')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2cb94b67-679e-4433-98ff-6513be58ad59 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2cb94b67-679e-4433-98ff-6513be58ad59');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "genotypes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gFMrUnhV80n"
      },
      "outputs": [],
      "source": [
        "ped_file = 'integrated_call_samples.20130502.ALL.ped'\n",
        "pedigree = pd.read_csv(ped_file, sep='\\t', index_col='Individual ID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "Zgb8Fn-EV_PU",
        "outputId": "f1838c6d-8f3e-4adc-9521-4c47bd2c049b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              Family ID Paternal ID Maternal ID  Gender  Phenotype Population  \\\n",
              "Individual ID                                                                   \n",
              "HG00096         HG00096           0           0       1          0        GBR   \n",
              "HG00097         HG00097           0           0       2          0        GBR   \n",
              "HG00098         HG00098           0           0       1          0        GBR   \n",
              "HG00099         HG00099           0           0       2          0        GBR   \n",
              "HG00100         HG00100           0           0       2          0        GBR   \n",
              "\n",
              "              Relationship Siblings Second Order Third Order Children  \\\n",
              "Individual ID                                                           \n",
              "HG00096              unrel        0            0           0        0   \n",
              "HG00097              unrel        0            0           0        0   \n",
              "HG00098              unrel        0            0           0        0   \n",
              "HG00099              unrel        0            0           0        0   \n",
              "HG00100              unrel        0            0           0        0   \n",
              "\n",
              "              Other Comments  \n",
              "Individual ID                 \n",
              "HG00096                    0  \n",
              "HG00097                    0  \n",
              "HG00098                    0  \n",
              "HG00099                    0  \n",
              "HG00100                    0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7908e4dc-dfd0-4f76-a38c-cb0b7808216f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Family ID</th>\n",
              "      <th>Paternal ID</th>\n",
              "      <th>Maternal ID</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Phenotype</th>\n",
              "      <th>Population</th>\n",
              "      <th>Relationship</th>\n",
              "      <th>Siblings</th>\n",
              "      <th>Second Order</th>\n",
              "      <th>Third Order</th>\n",
              "      <th>Children</th>\n",
              "      <th>Other Comments</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Individual ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>HG00096</th>\n",
              "      <td>HG00096</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>GBR</td>\n",
              "      <td>unrel</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HG00097</th>\n",
              "      <td>HG00097</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>GBR</td>\n",
              "      <td>unrel</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HG00098</th>\n",
              "      <td>HG00098</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>GBR</td>\n",
              "      <td>unrel</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HG00099</th>\n",
              "      <td>HG00099</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>GBR</td>\n",
              "      <td>unrel</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HG00100</th>\n",
              "      <td>HG00100</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>GBR</td>\n",
              "      <td>unrel</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7908e4dc-dfd0-4f76-a38c-cb0b7808216f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7908e4dc-dfd0-4f76-a38c-cb0b7808216f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7908e4dc-dfd0-4f76-a38c-cb0b7808216f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "pedigree.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIfDLdvCV_Gx"
      },
      "outputs": [],
      "source": [
        "# pedigree['Super Population'] = pedigree['Population']\n",
        "# pedigree['Super Population'] = pedigree['Super Population'].replace({\n",
        "#     'YRI': 'AFR', 'CEU': 'EUR', 'GWD': 'AFR', 'ESN': 'AFR',\n",
        "#     'CHS': 'EAS', 'IBS': 'EUR', 'PJL': 'SAS', 'PUR': 'AMR',\n",
        "#     'CLM': 'AMR', 'BEB': 'SAS', 'CHB': 'EAS', 'PEL': 'AMR',\n",
        "#     'STU': 'SAS', 'MSL': 'AFR', 'JPT': 'EAS', 'KHV': 'EAS',\n",
        "#     'ACB': 'AMR', 'LWK': 'AFR', 'ITU': 'SAS', 'GIH': 'SAS',\n",
        "#     'ASW': 'AMR', 'TSI': 'EUR', 'CDX': 'EAS', 'CHD': 'EAS',\n",
        "#     'GBR': 'EUR', 'MXL': 'AMR', 'FIN': 'EUR' })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDmXwq4uV_EP"
      },
      "outputs": [],
      "source": [
        "# pedigree['Super Population'] = pedigree['Super Population'].replace({\n",
        "#     'EAS': 0,\n",
        "#     'AMR': 1,\n",
        "#     'EUR': 2,\n",
        "#     'AFR': 3,\n",
        "#     'SAS': 4\n",
        "# })\n",
        "# pedigree.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBSfo4FlV_A_",
        "outputId": "e2be332e-60b9-4d27-a334-72e01891fbf7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2504,)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "Y_train = pedigree.loc[genotypes.index]['Population']\n",
        "# Y_train = pedigree.loc[genotypes.index][pedigree.loc[genotypes.index]['Population'] == \"YRI\"]['Population']\n",
        "# Y_train = pd.concat((Y_train, Y_train))\n",
        "Y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiP0EEwsEMNP",
        "outputId": "b5f79061-4ba0-4b8c-b6ef-807ae8c532ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2504, 7161)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "X = genotypes[genotypes.index.isin(Y_train.index)]\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNU6deVHXIdE",
        "outputId": "90b9ad39-8fe9-40d0-8192-3e17d3fa619e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2504, 7161)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# X1 = X.replace({\n",
        "#     '0|0': 0,\n",
        "#     '0|1': 0,\n",
        "#     '1|0': 1,\n",
        "#     '1|1': 1\n",
        "# })#.to_numpy()\n",
        "# X2 = X.replace({\n",
        "#     '0|0': 0,\n",
        "#     '0|1': 1,\n",
        "#     '1|0': 0,\n",
        "#     '1|1': 1\n",
        "# })\n",
        "# X = pd.concat((X1, X2))\n",
        "X = X.replace({\n",
        "    '0|0': 0,\n",
        "    '0|1': 1,\n",
        "    '1|0': 2,\n",
        "    '1|1': 3\n",
        "})\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFpb329KbMae"
      },
      "outputs": [],
      "source": [
        "# X = to_categorical(X, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pZoO-FvKdr3"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "feature_size = X.shape[1]\n",
        "inChannel = 3\n",
        "learning_rate = 0.01\n",
        "weight_decay = 0.00001\n",
        "embed_dim = 64  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "regularization_coef_l1 = 1e-4\n",
        "batch_size = 20\n",
        "dropout_rate = 0.25\n",
        "attention_range = 400\n",
        "chunk_size = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtSxW2eMOOCU"
      },
      "source": [
        "## Convert to tensorflow dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDiVoe67JHcy"
      },
      "outputs": [],
      "source": [
        "@tf.function()\n",
        "def add_attention_mask(X_sample, y_sample):\n",
        "  depth = 3\n",
        "  # all_indices = tf.where(tf.math.reduce_any(tf.reshape(X_sample, (-1, 1)) > -1, axis=1))\n",
        "  mask_size = tf.cast(X_sample.shape[0]*0.5, dtype=tf.int64)\n",
        "  mask_idx = tf.reshape(tf.random.shuffle(tf.range(X_sample.shape[0]))[:mask_size], (-1, 1))\n",
        "  updates = tf.math.add(tf.ones(shape=(mask_idx.shape[0]), dtype=tf.int64), 1)\n",
        "  X_masked = tf.tensor_scatter_nd_update(X_sample, mask_idx, updates)\n",
        "  # label_noise = tf.random.uniform(shape=[y_sample.shape[0]], minval=-.1, maxval=.1)\n",
        "  # noisy_labels = tf.math.add(tf.cast(y_sample, tf.float32), label_noise)\n",
        "\n",
        "  # return tf.one_hot(tf.expand_dims(X_masked, 0), depth), tf.one_hot(tf.expand_dims(y_sample, 0), depth)\n",
        "  return tf.one_hot(X_masked, depth), tf.one_hot(y_sample, depth)\n",
        "  # return tf.one_hot(X_masked, depth), y_sample\n",
        "  # return X_masked, y_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlyxYCy96H7V"
      },
      "outputs": [],
      "source": [
        "def map_values_1(x):\n",
        "  return 0 if (x == 0 or x == 1) else 1\n",
        "\n",
        "def map_values_2(x):\n",
        "  return 0 if (x == 0 or x == 2) else 1\n",
        "\n",
        "def get_dataset(x, chunk_start, chunk_end, start_offset, end_offset, batch_size, training=True):\n",
        "  AUTO = tf.data.AUTOTUNE\n",
        "\n",
        "  _x = np.empty((x.shape[0] * 2, chunk_end-chunk_start), dtype=x.dtype)\n",
        "\n",
        "  map_values_1_vec = np.vectorize(map_values_1)\n",
        "  map_values_2_vec = np.vectorize(map_values_2)\n",
        "\n",
        "  _x[0::2] = map_values_1_vec(x[:, chunk_start:chunk_end])\n",
        "  _x[1::2] = map_values_2_vec(x[:, chunk_start:chunk_end])\n",
        "  new_chunk_end = _x.shape[1]\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((_x,\n",
        "                                                _x[:, start_offset:new_chunk_end-end_offset]))\n",
        "  # # Add Attention Mask\n",
        "  # dataset = dataset.map(add_attention_mask, num_parallel_calls=AUTO, deterministic=True)\n",
        "  \n",
        "  # dataset = dataset.batch(batch_size, drop_remainder=True, num_parallel_calls=AUTO)\n",
        "  \n",
        "  if training:\n",
        "    dataset = dataset.shuffle(_x.shape[0], reshuffle_each_iteration=True)\n",
        "    dataset = dataset.repeat()\n",
        "  \n",
        "  # Add Attention Mask\n",
        "  dataset = dataset.map(add_attention_mask, num_parallel_calls=AUTO, deterministic=False)\n",
        "\n",
        "  # Prefetech to not map the whole dataset\n",
        "  dataset = dataset.prefetch(AUTO)\n",
        "\n",
        "  dataset = dataset.batch(batch_size, drop_remainder=True, num_parallel_calls=AUTO)\n",
        "\n",
        "  # Always have a batch ready\n",
        "  # dataset = dataset.prefetch(1)\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWFwQFNh_hkZ"
      },
      "outputs": [],
      "source": [
        "# attention_mask = np.zeros((feature_size, feature_size), dtype='int32')\n",
        "# for i in range(feature_size):\n",
        "#   attention_indices = np.arange(max(0, i-attention_range), min(feature_size, i+attention_range))\n",
        "#   attention_mask[i, attention_indices] = 1\n",
        "# plt.imshow(attention_mask[:1000,:1000], cmap='hot', interpolation='nearest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpRUjMp_L914"
      },
      "source": [
        "## Custom Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9h9OjeMZ8_XU"
      },
      "outputs": [],
      "source": [
        "class CrossAttentionLayer(layers.Layer):\n",
        "  def __init__(self, local_dim, global_dim, activation=tf.nn.gelu, dropout_rate=0.1,):\n",
        "    super(CrossAttentionLayer, self).__init__()\n",
        "    self.local_dim = local_dim\n",
        "    self.global_dim = global_dim\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.activation = activation\n",
        "    self.layer_norm00 = layers.LayerNormalization()\n",
        "    self.layer_norm01 = layers.LayerNormalization()\n",
        "    self.layer_norm1 = layers.LayerNormalization()\n",
        "    self.ffn = tf.keras.Sequential(\n",
        "          [\n",
        "            layers.Dense(self.local_dim//2, activation=self.activation, \n",
        "                        ),\n",
        "            layers.Dense(self.local_dim, \n",
        "                        activation=self.activation,\n",
        "                        ), ]\n",
        "      )\n",
        "    self.query_encoder = layers.Dense(units=self.local_dim)\n",
        "    self.key_encoder = layers.Dense(units=self.global_dim)\n",
        "    self.value_encoder = layers.Dense(units=self.global_dim)\n",
        "    self.add0 = layers.Add()\n",
        "    self.add1 = layers.Add()\n",
        "    self.attention = layers.Attention(use_scale=True, dropout=self.dropout_rate)\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    local_repr = self.layer_norm00(inputs[0])\n",
        "    global_repr = self.layer_norm01(inputs[1])\n",
        "    # Create query tensor: [1, latent_dim, projection_dim].\n",
        "    query = self.query_encoder(local_repr)\n",
        "    # Create key tensor: [batch_size, data_dim, projection_dim].\n",
        "    key = self.key_encoder(global_repr)\n",
        "    # Create value tensor: [batch_size, data_dim, projection_dim].\n",
        "    value = self.value_encoder(global_repr)\n",
        "\n",
        "    # Generate cross-attention outputs: [batch_size, latent_dim, projection_dim].\n",
        "    attention_output = self.attention(\n",
        "        [query, key, value], return_attention_scores=False\n",
        "    )\n",
        "    # Skip connection 1.\n",
        "    attention_output = self.add0([attention_output, query])\n",
        "\n",
        "    # Apply layer norm.\n",
        "    attention_output = self.layer_norm1(attention_output)\n",
        "    # Apply Feedforward network.\n",
        "    outputs = self.ffn(attention_output)\n",
        "    # Skip connection 2.\n",
        "    outputs = self.add1([outputs, attention_output])\n",
        "    return outputs\n",
        "\n",
        "class MaskedTransformerBlock(layers.Layer):\n",
        "  def __init__(self, embed_dim, num_heads, ff_dim, attention_range, start_offset=0, end_offset=0, attn_block_repeats=1, activation=tf.nn.gelu, dropout_rate=0.1, use_ffn=True):\n",
        "    super(MaskedTransformerBlock, self).__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.ff_dim = ff_dim\n",
        "    self.start_offset = start_offset\n",
        "    self.end_offset = end_offset\n",
        "    self.attention_range = attention_range\n",
        "    self.attn_block_repeats = attn_block_repeats\n",
        "    self.activation = activation\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.use_ffn = use_ffn\n",
        "    self.att0 = [layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_dim, dropout=self.dropout_rate) for _ in range(attn_block_repeats)]\n",
        "    if self.use_ffn:\n",
        "      self.ffn = [tf.keras.Sequential(\n",
        "          [\n",
        "            layers.Dense(self.ff_dim, activation=self.activation, \n",
        "                        ),\n",
        "            layers.Dense(self.embed_dim, \n",
        "                        activation=self.activation,\n",
        "                        ), ]\n",
        "      ) for _ in range(attn_block_repeats)]\n",
        "    self.layer_norm0 = [layers.LayerNormalization() for _ in range(attn_block_repeats)]\n",
        "    self.layer_norm1 = [layers.LayerNormalization() for _ in range(attn_block_repeats)]\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    assert(self.end_offset >= 0)\n",
        "    self.feature_size = input_shape[1]\n",
        "    attention_mask = np.zeros((self.feature_size,\n",
        "                               self.feature_size), dtype=bool)\n",
        "    for i in range(self.start_offset, self.feature_size - self.end_offset):\n",
        "      attention_indices = np.arange(max(0, i-self.attention_range), min(self.feature_size, i+self.attention_range))\n",
        "      attention_mask[i, attention_indices] = True\n",
        "    \n",
        "    # np.repeat(a.reshape((1, a.shape[0], a.shape[1])), 3, axis=0)\n",
        "    # self.attention_mask = attention_mask[self.start_offset:self.feature_size-self.end_offset]\n",
        "    # self.attention_mask = self.attention_mask.reshape((1, self.attention_mask.shape[0], self.attention_mask.shape[1]))\n",
        "    self.attention_mask = tf.constant(attention_mask[self.start_offset:self.feature_size-self.end_offset])\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    # # x = self.concat([inputs[0], inputs[1]])\n",
        "    # if type(inputs.shape[0]) is int:\n",
        "    #   attn_mask = tf.constant(np.repeat(self.attention_mask, inputs.shape[0], axis=0))\n",
        "    # else:\n",
        "    #   attn_mask = tf.constant(self.attention_mask.reshape((self.attention_mask.shape[1], self.attention_mask.shape[2])))\n",
        "    \n",
        "    x = inputs\n",
        "    for i in range(self.attn_block_repeats-1):\n",
        "      x = self.layer_norm0[i](x)\n",
        "      attn_output = self.att0[i](x, x)\n",
        "      out1 = x + attn_output\n",
        "      out1 = self.layer_norm1[i](out1)\n",
        "      if self.use_ffn:\n",
        "        ffn_output = self.ffn[i](out1)\n",
        "        x = out1 + ffn_output\n",
        "      else:\n",
        "        x = out1\n",
        "\n",
        "    x = self.layer_norm0[-1](inputs)\n",
        "    # x = inputs\n",
        "    # attn_output = self.att0(x, x)\n",
        "    attn_output = self.att0[-1](x[:, self.start_offset:x.shape[1]-self.end_offset, :], x,\n",
        "                            # attention_mask=tf.expand_dims(self.attention_mask, 0)\n",
        "                            )\n",
        "    # out1 = x + attn_output\n",
        "    out1 = x[:, self.start_offset:x.shape[1]-self.end_offset, :] + attn_output\n",
        "    out1 = self.layer_norm1[-1](out1)\n",
        "    if self.use_ffn:\n",
        "      ffn_output = self.ffn[-1](out1)\n",
        "      x = out1 + ffn_output\n",
        "    else:\n",
        "      x = out1\n",
        "    return x\n",
        "\n",
        "@tf.function\n",
        "def my_mat_mul(embedding, input):\n",
        "  y = tf.einsum('ijk,jkl->ijl', input, embedding)\n",
        "  return y\n",
        "\n",
        "@tf.function\n",
        "def my_new_mat_mul(embedding, input):\n",
        "  return tf.einsum('ijk,kl->ijl', input, embedding)\n",
        "\n",
        "class NewGenoEmbeddings(layers.Layer):\n",
        "  def __init__(self, embedding_dim, \n",
        "               embeddings_initializer='glorot_uniform',\n",
        "               embeddings_regularizer=None,\n",
        "               activity_regularizer=None,\n",
        "               embeddings_constraint=None):\n",
        "    super(NewGenoEmbeddings, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.embeddings_initializer = initializers.get(embeddings_initializer)\n",
        "    self.embeddings_regularizer = regularizers.get(embeddings_regularizer)\n",
        "    self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "    self.embeddings_constraint = constraints.get(embeddings_constraint)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    # print(input_shape)\n",
        "    \n",
        "    self.num_of_allels = input_shape[-1]\n",
        "    self.n_snps = input_shape[-2]\n",
        "    self.position_embedding = layers.Embedding(\n",
        "            input_dim=self.n_snps, output_dim=self.embedding_dim\n",
        "        )\n",
        "    # self.projection = layers.Dense(units=self.embedding_dim)\n",
        "    self.embedding = self.add_weight(\n",
        "            shape=(self.num_of_allels, self.embedding_dim),\n",
        "            initializer=self.embeddings_initializer,\n",
        "            trainable=True, name='geno_embeddings',\n",
        "            regularizer=self.embeddings_regularizer,\n",
        "            constraint=self.embeddings_constraint,\n",
        "            experimental_autocast=False\n",
        "        )\n",
        "    self.positions = tf.range(start=0, limit=self.n_snps, delta=1)\n",
        "    # self.matmul_calculator = MyMatmul()\n",
        "    # self.myEinSumLayer = layers.Lambda(lambda x: tf.einsum('ijk,kl->ijl',x[0], x[1]))\n",
        "  def call(self, inputs):\n",
        "    # return self.projection(inputs) + self.position_embedding(positions)\n",
        "    self.immediate_result = tf.einsum('ijk,kl->ijl', inputs, self.embedding)\n",
        "    return self.immediate_result + self.position_embedding(self.positions)\n",
        "\n",
        "class GenoEmbeddings(layers.Layer):\n",
        "  def __init__(self, embedding_dim, \n",
        "               embeddings_initializer='glorot_uniform',\n",
        "               embeddings_regularizer=None,\n",
        "               activity_regularizer=None,\n",
        "               embeddings_constraint=None):\n",
        "    super(GenoEmbeddings, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.embeddings_initializer = initializers.get(embeddings_initializer)\n",
        "    self.embeddings_regularizer = regularizers.get(embeddings_regularizer)\n",
        "    self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "    self.embeddings_constraint = constraints.get(embeddings_constraint)\n",
        "\n",
        "  \n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "    config.update({\n",
        "        'embedding_dim': self.embedding_dim,\n",
        "        'embeddings_initializer': self.embeddings_initializer,\n",
        "        'embeddings_regularizer': self.embeddings_regularizer,\n",
        "        'activity_regularizer': self.activity_regularizer,\n",
        "        'embeddings_constraint': self.embeddings_constraint,\n",
        "        # 'att0': self.att0,\n",
        "        # 'ffn': self.ffn,\n",
        "        # 'dropout1': self.dropout1,\n",
        "        # 'dropout2': self.dropout2\n",
        "    })\n",
        "    return config\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    # print(input_shape)\n",
        "    self.num_of_allels = input_shape[-1]\n",
        "    self.n_snps = input_shape[-2]\n",
        "    self.embedding = self.add_weight(\n",
        "            shape=(self.n_snps, self.num_of_allels, self.embedding_dim),\n",
        "            initializer=self.embeddings_initializer,\n",
        "            trainable=True, name='geno_embeddings',\n",
        "            regularizer=self.embeddings_regularizer,\n",
        "            constraint=self.embeddings_constraint,\n",
        "            experimental_autocast=False\n",
        "        )\n",
        "    # self.position_embedding = layers.Embedding(\n",
        "    #         input_dim=self.n_snps, output_dim=self.embedding_dim\n",
        "    #     )\n",
        "  def call(self, inputs):\n",
        "    # positions = tf.range(start=0, limit=self.n_snps, delta=1)\n",
        "    return my_mat_mul(self.embedding, inputs)# + self.position_embedding(positions)\n",
        "\n",
        "class Chunker(layers.Layer):\n",
        "  def __init__(self, embed_dim, num_heads, ff_dim, chk_size=chunk_size,\n",
        "               activation=tf.nn.gelu, dropout_rate=0.25, attn_block_repeats=1,\n",
        "               attention_range=attention_range, include_embedding_layer=False):\n",
        "    super(Chunker, self).__init__()\n",
        "    self.concat = layers.Concatenate(axis=-2)\n",
        "    self.chunk_size = chk_size\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.ff_dim = ff_dim\n",
        "    self.activation = activation\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.attention_range = attention_range\n",
        "    self.attn_block_repeats = attn_block_repeats\n",
        "    self.include_embedding_layer = include_embedding_layer\n",
        "    # self.ln = layers.LayerNormalization()\n",
        "\n",
        "    # self.num_conv_layers = 2\n",
        "    # self.num_output_channels = [embed_dim//2, embed_dim]\n",
        "    # self.kernel_size = 3\n",
        "    # self.stride = 1\n",
        "    # self.padding = 1\n",
        "    # self.pooling_kernel_size = 3\n",
        "    # self.pooling_stride = 2\n",
        "\n",
        "    # self.conv_model = keras.Sequential()\n",
        "    # for i in range(self.num_conv_layers):\n",
        "    #   self.conv_model.add(\n",
        "    #       layers.Conv1D(\n",
        "    #           self.num_output_channels[i],\n",
        "    #           self.kernel_size,\n",
        "    #           self.stride,\n",
        "    #           padding=\"valid\",\n",
        "    #           use_bias=False,\n",
        "    #           activation=tf.nn.gelu,\n",
        "    #           kernel_initializer=\"he_normal\",\n",
        "    #       )\n",
        "    #   )\n",
        "    #   # self.conv_model.add(layers.ZeroPadding1D(self.padding))\n",
        "    #   self.conv_model.add(\n",
        "    #       layers.MaxPool1D(self.pooling_kernel_size, self.pooling_stride, \"same\")\n",
        "    #   )\n",
        "    # # self.conv_model.add(layers.BatchNormalization())\n",
        "    # # self.conv_model.add(layers.Dropout(self.dropout_rate))\n",
        "    \n",
        "  def build(self, input_shape):\n",
        "    self.chunk_starts = list(range(0, input_shape[1], self.chunk_size))\n",
        "    self.chunk_ends = []\n",
        "    for cs in self.chunk_starts:\n",
        "      self.chunk_ends.append(min(cs+self.chunk_size, input_shape[1]))\n",
        "    self.mask_starts = [max(0, cs-self.attention_range) for cs in self.chunk_starts]\n",
        "    self.mask_ends = [min(ce+self.attention_range, input_shape[1]) for ce in self.chunk_ends]\n",
        "    self.chunkers = [Chunk(self.embed_dim, self.num_heads, self.ff_dim,\n",
        "                           attention_range,\n",
        "                           include_embedding_layer=self.include_embedding_layer,\n",
        "                           start_offset=cs - self.mask_starts[i], \n",
        "                            end_offset=self.mask_ends[i]-self.chunk_ends[i],\n",
        "                           attn_block_repeats=self.attn_block_repeats) for i, cs in enumerate(self.chunk_starts)]\n",
        "    # self.cross_attentions = [CrossAttentionLayer(self.embed_dim, self.embed_dim) for i, cs in enumerate(self.chunk_starts)]\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    x = inputs\n",
        "    # compact_repr = self.conv_model(x)\n",
        "    # print(compact_repr.shape)\n",
        "    chunks = [chunker(x[:, self.mask_starts[i]:self.mask_ends[i]]) for i, chunker in enumerate(self.chunkers)]\n",
        "    y = self.concat(chunks)\n",
        "    # compact_repr = self.conv_model(y)\n",
        "    # chunks = [self.cross_attentions[i]([y[:, self.chunk_starts[i]:self.chunk_ends[i]], compact_repr]) for i, chunker in enumerate(self.chunkers)]\n",
        "    # chunks = [chunker(x[:, self.mask_starts[i]:self.mask_ends[i]]) + \n",
        "    #           self.cross_attentions[i]([x[:, self.chunk_starts[i]:self.chunk_ends[i]], compact_repr]) for i, chunker in enumerate(self.chunkers)]\n",
        "    # chunks_reloaded = [cross_attention([x[:, self.chunk_starts[i]:self.chunk_ends[i]], compact_repr]) for i, cross_attention in enumerate(self.cross_attentions)]\n",
        "    # y = [chunks[i] + chunks_reloaded[i] for i in range(len(self.chunk_starts))]\n",
        "    # y = self.concat(chunks)\n",
        "    # y = self.ln(y)\n",
        "    return y\n",
        "\n",
        "\n",
        "class Chunk(layers.Layer):\n",
        "  def __init__(self, embed_dim, num_heads, ff_dim, attention_range,\n",
        "               start_offset=0, end_offset=0,\n",
        "               attn_block_repeats=1,\n",
        "               include_embedding_layer=False):\n",
        "    super(Chunk, self).__init__()\n",
        "    self.attention_range = attention_range\n",
        "    self.ff_dim = ff_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.embed_dim = embed_dim\n",
        "    self.attn_block_repeats = attn_block_repeats\n",
        "    self.include_embedding_layer = include_embedding_layer\n",
        "\n",
        "    self.attention_block = MaskedTransformerBlock(self.embed_dim,\n",
        "                                                   self.num_heads, self.ff_dim,\n",
        "                                                   attention_range, start_offset,\n",
        "                                                   end_offset, attn_block_repeats=1)\n",
        "    if include_embedding_layer:\n",
        "      self.embedding = NewGenoEmbeddings(embed_dim)\n",
        "    \n",
        "\n",
        "  def build(self, input_shape):\n",
        "    # print(input_shape, input_shape[1])\n",
        "    # self.latent_array = self.add_weight(\n",
        "    #         shape=(input_shape[1] - self.end_offset - self.start_offset, self.embed_dim),\n",
        "    #         initializer=\"random_normal\",\n",
        "    #         trainable=True,\n",
        "    #     )\n",
        "    # Create cross-attenion module.\n",
        "    # self.cross_attention = create_cross_attention_module(\n",
        "    #     input_shape[1] - self.end_offset - self.start_offset,\n",
        "    #     input_shape[1],\n",
        "    #     self.embed_dim,\n",
        "    #     self.embed_dim // 2,\n",
        "    #     self.dropout_rate,\n",
        "    # )\n",
        "    pass\n",
        "  \n",
        "  def call(self, inputs, training):\n",
        "    if self.include_embedding_layer:\n",
        "      x = self.embedding(inputs)\n",
        "    else:\n",
        "      x = inputs\n",
        "    # for i in range(self.attn_block_repeats):\n",
        "    x = self.attention_block(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waFVuh7DMCkM"
      },
      "source": [
        "## Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FgZf0fOb5yG"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(layers.Layer):\n",
        "  def __init__(self, embed_dim):\n",
        "    super(ConvBlock, self).__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    # self.dense0 = layers.Dense(embed_dim, activation=tf.nn.gelu)\n",
        "    self.conv000 = layers.Conv1D(embed_dim, 3, padding='same', activation=tf.nn.gelu,\n",
        "                    # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "                    )\n",
        "    self.conv010 = layers.Conv1D(embed_dim, 5, padding='same', activation=tf.nn.gelu,\n",
        "                    # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "                    )\n",
        "    self.conv011 = layers.Conv1D(embed_dim, 7, padding='same', activation=tf.nn.gelu,\n",
        "                    # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "                    )\n",
        "    \n",
        "    self.conv020 = layers.Conv1D(embed_dim, 7, padding='same', activation=tf.nn.gelu,\n",
        "                    # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "                    )\n",
        "    self.conv021 = layers.Conv1D(embed_dim, 15, padding='same', activation=tf.nn.gelu,\n",
        "                    # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "                    )\n",
        "    self.add = layers.Add()\n",
        "    self.conv100 = layers.Conv1D(embed_dim, 3, padding='same', activation=tf.nn.gelu,)\n",
        "    self.bn0 = layers.BatchNormalization()\n",
        "    self.bn1 = layers.BatchNormalization()\n",
        "    self.dw_conv = layers.DepthwiseConv1D(embed_dim, 1, padding='same')\n",
        "    self.activation = layers.Activation(tf.nn.gelu)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Could add skip connection here?\n",
        "    xa = self.conv000(inputs)\n",
        "    \n",
        "    # xa0 = self.dense0(inputs)\n",
        "    xb = self.conv010(xa)\n",
        "    xb = self.conv011(xb)\n",
        "\n",
        "    xc = self.conv020(xa)\n",
        "    xc = self.conv021(xc)\n",
        "    xa = self.add([xb, xc])\n",
        "    xa = self.conv100(xa)\n",
        "    xa = self.bn0(xa)\n",
        "    xa = self.dw_conv(xa)\n",
        "    xa = self.bn1(xa)\n",
        "    xa = self.activation(xa)\n",
        "    return xa\n",
        "\n",
        "def chunk_module(embed_dim, num_heads, input_len, input_channels, attention_range,\n",
        "               start_offset=0, end_offset=0,\n",
        "               attn_block_repeats=1, include_embedding=False):\n",
        "  projection_dim = embed_dim\n",
        "  inputs = layers.Input(shape=(input_len, input_channels))\n",
        "  xa = inputs\n",
        "  # if include_embedding:\n",
        "  xa = NewGenoEmbeddings(projection_dim)(xa)\n",
        "  # else:\n",
        "    # xa = layers.Conv1D(embed_dim, 5, padding='same', activation=tf.nn.gelu)(xa) \n",
        "  xa = Chunk(projection_dim, num_heads, projection_dim//2, attention_range, 0, 0, 1, include_embedding_layer=False)(xa)\n",
        "  \n",
        "  xa = ConvBlock(projection_dim)(xa)\n",
        "  projection_dim = int(projection_dim//2)\n",
        "  xa = ConvBlock(projection_dim)(xa)\n",
        "  # xa = layers.Dropout(0.2)(xa)\n",
        "  \n",
        "  xa0 = Chunk(projection_dim, num_heads, projection_dim//2, attention_range, start_offset, end_offset, 1, include_embedding_layer=False)(xa)\n",
        "  xa = layers.Conv1D(projection_dim, 5, padding='same', activation=tf.nn.gelu,)(xa0)\n",
        "  xa = layers.Add()([xa0, xa])\n",
        "  xa = layers.BatchNormalization()(xa)\n",
        "\n",
        "  xa = layers.Conv1D(inChannel, 5, padding='same', activation=tf.nn.softmax)(xa)\n",
        "  \n",
        "  model = keras.Model(inputs=inputs, outputs=xa)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD7WOyyoNYGv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6914328-42ee-4b1d-e9a1-ef819e63e0bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "num_classes = len(np.unique(Y_train))\n",
        "num_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCkow4q5MJk_"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgGz9_6u8u-O"
      },
      "outputs": [],
      "source": [
        "class SplitTransformer(keras.Model):\n",
        "  def __init__(\n",
        "      self,\n",
        "      embed_dim,\n",
        "      num_heads,\n",
        "      latent_dim = 512,\n",
        "      chunk_size=chunk_size,\n",
        "      activation=tf.nn.gelu,\n",
        "      dropout_rate=0.25,\n",
        "      attn_block_repeats=1,\n",
        "      attention_range=attention_range):\n",
        "    super(SplitTransformer, self).__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.chunk_size = chunk_size\n",
        "    self.activation = activation\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.attn_block_repeats = attn_block_repeats\n",
        "    self.attention_range = attention_range\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    # Create latent array.\n",
        "    # self.latent_array = self.add_weight(\n",
        "    #     shape=(self.latent_dim, self.projection_dim),\n",
        "    #     initializer=\"random_normal\",\n",
        "    #     trainable=True,\n",
        "    # )\n",
        "    \n",
        "    \n",
        "    self.chunk_starts = list(range(0, input_shape[1], self.chunk_size))\n",
        "    self.chunk_ends = []\n",
        "    for cs in self.chunk_starts:\n",
        "      self.chunk_ends.append(min(cs+self.chunk_size, input_shape[1]))\n",
        "    self.mask_starts = [max(0, cs-self.attention_range) for cs in self.chunk_starts]\n",
        "    self.mask_ends = [min(ce+self.attention_range, input_shape[1]) for ce in self.chunk_ends]\n",
        "    # self.chunkers = [Chunk(self.embed_dim, self.num_heads, self.ff_dim,\n",
        "    #                        attention_range,\n",
        "    #                        include_embedding_layer=self.include_embedding_layer,\n",
        "    #                        start_offset=cs - self.mask_starts[i], \n",
        "    #                         end_offset=self.mask_ends[i]-self.chunk_ends[i],\n",
        "    #                        attn_block_repeats=self.attn_block_repeats) for i, cs in enumerate(self.chunk_starts)]\n",
        "    self.chunkers = [chunk_module(self.embed_dim, self.num_heads,\n",
        "                                  self.mask_ends[i] - self.mask_starts[i],\n",
        "                                  inChannel, self.attention_range,\n",
        "                                  start_offset=cs - self.mask_starts[i],\n",
        "                                  end_offset=self.mask_ends[i]-self.chunk_ends[i],\n",
        "                                  attn_block_repeats=1, include_embedding=True) for i,cs in enumerate(self.chunk_starts)]\n",
        "\n",
        "    self.concat_layer = layers.Concatenate(axis=-2)\n",
        "    # self.immediate_layers = layers.Conv1D(inChannel, 5, padding='same', activation=tf.nn.gelu)\n",
        "    # self.output_layer = layers.Conv1D(inChannel, 3, padding='same', activation=tf.nn.softmax)\n",
        "    super(SplitTransformer, self).build(input_shape)\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    chunks = [self.chunkers[i](inputs[:, self.mask_starts[i]:self.mask_ends[i]]) for i, chunker in enumerate(self.chunkers)]\n",
        "    y = self.concat_layer(chunks)\n",
        "    # y = self.chunker(inputs[:, self.mask_starts[0]:self.mask_ends[0]])\n",
        "    # y = self.output_layer(y)\n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWFbNNF4i4HD"
      },
      "outputs": [],
      "source": [
        "def create_model(embedding_size=512, chunk_size=chunk_size, attn_block_repeats=1):\n",
        "  return SplitTransformer(embedding_size,\n",
        "      num_heads,\n",
        "      latent_dim = 512,\n",
        "      attn_block_repeats=attn_block_repeats,\n",
        "      chunk_size=chunk_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69stM-Pzhskh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570aae32-270d-4a59-a8bb-ec93d51d9114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"split_transformer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model (Functional)          (None, 1000, 3)           17652867  \n",
            "                                                                 \n",
            " model_1 (Functional)        (None, 1000, 3)           17857667  \n",
            "                                                                 \n",
            " model_2 (Functional)        (None, 1000, 3)           17857667  \n",
            "                                                                 \n",
            " model_3 (Functional)        (None, 1000, 3)           17857667  \n",
            "                                                                 \n",
            " model_4 (Functional)        (None, 1000, 3)           17857667  \n",
            "                                                                 \n",
            " model_5 (Functional)        (None, 1000, 3)           17857667  \n",
            "                                                                 \n",
            " model_6 (Functional)        (None, 1000, 3)           17735299  \n",
            "                                                                 \n",
            " model_7 (Functional)        (None, 161, 3)            17223299  \n",
            "                                                                 \n",
            " concatenate (Concatenate)   multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 141,899,800\n",
            "Trainable params: 141,871,128\n",
            "Non-trainable params: 28,672\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = create_model(512, attn_block_repeats=1)\n",
        "model.build((1, feature_size, inChannel))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skB-jC5IZOBN"
      },
      "outputs": [],
      "source": [
        "# tf.keras.utils.plot_model(model, show_shapes=True, expand_nested=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mv32H3-y75Fo"
      },
      "outputs": [],
      "source": [
        "def get_three_sets(x, y, population_label, missing_perc):\n",
        "  # np.random.seed(seed=random_state)\n",
        "\n",
        "  \n",
        "#     {'YRI': 'AFR', 'CEU': 'EUR', 'GWD': 'AFR', 'ESN': 'AFR',\n",
        "#     'CHS': 'EAS', 'IBS': 'EUR', 'PJL': 'SAS', 'PUR': 'AMR',\n",
        "#     'CLM': 'AMR', 'BEB': 'SAS', 'CHB': 'EAS', 'PEL': 'AMR',\n",
        "#     'STU': 'SAS', 'MSL': 'AFR', 'JPT': 'EAS', 'KHV': 'EAS',\n",
        "#     'ACB': 'AMR', 'LWK': 'AFR', 'ITU': 'SAS', 'GIH': 'SAS',\n",
        "#     'ASW': 'AMR', 'TSI': 'EUR', 'CDX': 'EAS', 'CHD': 'EAS',\n",
        "#     'GBR': 'EUR', 'MXL': 'AMR', 'FIN': 'EUR' }\n",
        "#     to_categorical(X, 4)\n",
        "  # _x = to_categorical(x[x.index.isin(y.index)].to_numpy(), inChannel)\n",
        "  _x = x[x.index.isin(y.index)].to_numpy()\n",
        "  # _sx = s_X_train.to_numpy()\n",
        "  _y = y.to_numpy()\n",
        "  # x_test = to_categorical(x[x.index.isin(y[y.values == population_label].index)].to_numpy(), 4)\n",
        "  # y_test = y[y.values == population_label].to_numpy()\n",
        "  # x_train = to_categorical(x[x.index.isin(y[y.values != population_label].index)].to_numpy(), 4)\n",
        "  # y_train = y[y.values != population_label].to_numpy()\n",
        "  x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.20,\n",
        "                                        random_state=random_state,\n",
        "                                        shuffle=True,\n",
        "                                        stratify=_y,)\n",
        "  train_indices, test_indices = train_test_split(y.index, test_size=0.20,\n",
        "                                        random_state=random_state,\n",
        "                                        shuffle=True,\n",
        "                                        stratify=_y,)\n",
        "  # x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.19,\n",
        "  #                                             random_state=random_state,\n",
        "  #                                             shuffle=True,\n",
        "  #                                             stratify=y_train,\n",
        "  #                                                     )\n",
        "  # steps_per_epoch = x_train.shape[0] // batch_size\n",
        "  # validation_steps = x_valid.shape[0] // batch_size\n",
        "  # print(f\"x_train percentage: {x_train.shape[0]/x.shape[0]}\")\n",
        "  # print(f\"x_valid percentage: {x_valid.shape[0]/x.shape[0]}\")\n",
        "  # print(f\"x_test percentage: {x_test.shape[0]/x.shape[0]}\")\n",
        "  print(f\"x_train shape: {x_train.shape}\")\n",
        "  # print(f\"x_valid shape: {x_valid.shape}\")\n",
        "  print(f\"x_test shape: {x_test.shape}\")\n",
        "  # DataGenerator(x_train, mask, y_train, batch_size, missing_perc=missing_perc)\n",
        "  return x_train, y_train, (x_test, y_test), test_indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CVnIE7HK63r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab42a0c-15a6-4d02-d0a1-dd3d3433ddd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ACB', 'ASW', 'BEB', 'CDX', 'CEU', 'CHB', 'CHS', 'CLM', 'ESN',\n",
              "       'FIN', 'GBR', 'GIH', 'GWD', 'IBS', 'ITU', 'JPT', 'KHV', 'LWK',\n",
              "       'MSL', 'MXL', 'PEL', 'PJL', 'PUR', 'STU', 'TSI', 'YRI'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "np.unique(Y_train.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doSuiA3dyTyg"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIsw5pOFyhws"
      },
      "outputs": [],
      "source": [
        "# save_path = \"/content/drive/MyDrive/Colab Notebooks/TestOutput\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61EgnuNmoFLT"
      },
      "outputs": [],
      "source": [
        "METRIC = \"val_loss\"\n",
        "\n",
        "def create_callbacks(kfold=0, metric = METRIC):\n",
        "    \n",
        "    # cpk_path = f'./drive/My Drive/ShiLab/training_checkpoints/HLA/mixed_pop/mixed_best_model_{kfold}.h5'\n",
        "    \n",
        "    # checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    #     filepath=cpk_path,\n",
        "    #     monitor= metric,\n",
        "    #     mode='min',\n",
        "    #     save_best_only=True,\n",
        "    #     verbose=1,\n",
        "    # )\n",
        "\n",
        "    reducelr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor= metric,\n",
        "        mode='min',\n",
        "        factor=0.2,\n",
        "        patience=4,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    earlystop = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor= metric,\n",
        "        mode='min',\n",
        "        patience= 10, \n",
        "        verbose=1,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "    \n",
        "    callbacks = [\n",
        "                #  checkpoint,\n",
        "                 reducelr,\n",
        "                 earlystop]         \n",
        "    \n",
        "    return callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOA_NexzN5Qq"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey5lpPElXlUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "affcf186-d121-49de-8036-09de87737520"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# A TPU V3-8 has 8 computing cores, the global batch size will be 1/16 x 8 = 8/128\n",
        "BATCH_SIZE_BASE = 4\n",
        "# Training configuration\n",
        "BATCH_SIZE = BATCH_SIZE_BASE * N_REPLICAS if TPU else 16\n",
        "BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "missing_perc = 0.1\n",
        "model_size = 512\n",
        "# N_SPLITS = 5\n",
        "# batch_size = 50\n",
        "accuracies = []\n",
        "for random_state in range(0, 1000, 1000):\n",
        "  for population_label in np.unique(Y_train.values)[:1]:\n",
        "    \n",
        "    print(f\"Training using seed {random_state}\")\n",
        "    # train_dataset, valid_dataset, test_dataset, steps_per_epoch, validation_steps = get_three_sets(X, attention_mask, Y_train, batch_size, population_label, missing_perc)\n",
        "    x_train, y_train, test_dataset, test_indices = get_three_sets(X, Y_train, population_label, missing_perc)\n",
        "    # kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=random_state)\n",
        "    # for fold,(x_train_ids,x_valid_ids) in enumerate(kfold.split(x_train,y_train)):\n",
        "    # print(f\"Fold: {fold}\")\n",
        "    # print(x_train.shape)\n",
        "    K.clear_session()\n",
        "    # print(x_train_ids.shape, x_valid_ids.shape)\n",
        "    # x_train_data = x_train[x_train_ids]\n",
        "    # x_valid_data = x_train[x_valid_ids]\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.20,\n",
        "                                        random_state=random_state,\n",
        "                                        shuffle=True,\n",
        "                                        stratify=y_train)\n",
        "    \n",
        "    num_epochs = 1000\n",
        "    steps_per_epoch = 2*x_train.shape[0]//BATCH_SIZE\n",
        "    validation_steps = 2*x_valid.shape[0]//BATCH_SIZE\n",
        "    # train_dataset = DataGenerator(x_train, batch_size, missing_perc=missing_perc)\n",
        "    # train_dataset = get_dataset(x_train, 0, feature_size, 0, 0, batch_size, repeats=5)\n",
        "    train_dataset = get_dataset(x_train, 0, feature_size, 0, 0, BATCH_SIZE)\n",
        "    valid_dataset = get_dataset(x_valid, 0, feature_size, 0, 0, BATCH_SIZE, training=False)\n",
        "    # train_dataset = get_dataset(x_train, 0, chunk_size+attention_range, 0, attention_range, BATCH_SIZE)\n",
        "    # valid_dataset = get_dataset(x_valid, 0, chunk_size+attention_range, 0, attention_range, BATCH_SIZE, training=False)\n",
        "    callbacks = create_callbacks()\n",
        "\n",
        "    with strategy.scope():\n",
        "      model = create_model(model_size, attn_block_repeats=1)\n",
        "      model.build((BATCH_SIZE, feature_size, inChannel))\n",
        "      # model.build((BATCH_SIZE, chunk_size+attention_range, inChannel))\n",
        "      optimizer = tfa.optimizers.LAMB(\n",
        "            learning_rate=learning_rate\n",
        "        )\n",
        "      # optimizer = tfa.optimizers.LazyAdam(learning_rate=learning_rate)\n",
        "      # model.compile(optimizer, steps_per_execution = steps_per_epoch//2)\n",
        "      model.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=tf.keras.metrics.CategoricalAccuracy())\n",
        "\n",
        "      history = model.fit(\n",
        "          train_dataset,\n",
        "          steps_per_epoch=steps_per_epoch,\n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=validation_steps,\n",
        "          epochs=num_epochs,\n",
        "          verbose=2,\n",
        "          callbacks=callbacks\n",
        "      )\n",
        "    for missing_perc in [\n",
        "                        #  0.01, 0.05,\n",
        "                         0.1,\n",
        "                        #  0.2\n",
        "                         ]:\n",
        "      save_name = f\"./drive/My Drive/ShiLab/HLA/mixed_pop/preds_mixed_mr_{missing_perc}_rs_{random_state}_.csv\"\n",
        "      avg_accuracy = []\n",
        "      preds = []\n",
        "      true_labels = []\n",
        "      \n",
        "      to_save_array = np.zeros((test_dataset[0].shape[0], test_dataset[0].shape[1]), dtype=object)\n",
        "      test_X_missing = np.empty((test_dataset[0].shape[0] * 2, test_dataset[0].shape[1]), dtype=test_dataset[0].dtype)\n",
        "      map_values_1_vec = np.vectorize(map_values_1)\n",
        "      map_values_2_vec = np.vectorize(map_values_2)\n",
        "      test_X_missing[0::2] = map_values_1_vec(test_dataset[0])\n",
        "      test_X_missing[1::2] = map_values_2_vec(test_dataset[0])\n",
        "      test_X_missing = to_categorical(test_X_missing, 3)\n",
        "      for i in tqdm(range(test_dataset[0].shape[0])):\n",
        "          rng = np.random.default_rng(i)\n",
        "          # Generates missing genotypes\n",
        "          missing_size = int(missing_perc * test_X_missing.shape[1])\n",
        "          missing_index = rng.integers(low=0, high=test_X_missing.shape[1],\n",
        "                                            size=missing_size)\n",
        "          test_X_missing[i*2:i*2+2, missing_index, :] = [0, 0, 1]\n",
        "          # mMask = np.tile(attention_mask, (1, 1, 1))\n",
        "          # predict\n",
        "          predict_onehot = model.predict(test_X_missing[i*2:i*2+2, :, :])\n",
        "          # predict_onehot = model.predict([test_X_missing[i:i + 1, :, :], mMask])\n",
        "          # only care the missing position\n",
        "          predict_missing_onehot = predict_onehot[0:2, missing_index, :]\n",
        "          # predict label\n",
        "          predict_missing = np.argmax(predict_missing_onehot, axis=2)\n",
        "          predict_missing_final = np.zeros((1, predict_missing.shape[1]))\n",
        "          for j in range(predict_missing.shape[1]):\n",
        "            if predict_missing[:, j].tolist() == [0, 0]:\n",
        "              predict_missing_final[:, j] = 0\n",
        "            elif predict_missing[:, j].tolist() == [0, 1]:\n",
        "              predict_missing_final[:, j] = 1\n",
        "            elif predict_missing[:, j].tolist() == [1, 0]:\n",
        "              predict_missing_final[:, j] = 2\n",
        "            elif predict_missing[:, j].tolist() == [1, 1]:\n",
        "              predict_missing_final[:, j] = 3\n",
        "            else:\n",
        "              predict_missing_final[:, j] = 4\n",
        "          preds.extend(predict_missing_final.ravel().tolist())\n",
        "          \n",
        "          predict_haplotypes = np.argmax(predict_onehot, axis=2)\n",
        "          for j in range(predict_onehot.shape[1]):\n",
        "            if predict_haplotypes[:, j].tolist() == [0,0]:\n",
        "              to_save_array[i, j] = '0|0'\n",
        "            elif predict_haplotypes[:, j].tolist() == [0,1]:\n",
        "              to_save_array[i, j] = '0|1'\n",
        "            elif predict_haplotypes[:, j].tolist() == [1,0]:\n",
        "              to_save_array[i, j] = '1|0'\n",
        "            elif predict_haplotypes[:, j].tolist() == [1, 1]:\n",
        "              to_save_array[i, j] = '1|1'\n",
        "            else:\n",
        "              to_save_array[i, j] = '.|.'\n",
        "          # real label\n",
        "          # label_missing_onehot = test_dataset[0][i:i + 1, missing_index, :]\n",
        "          # label_missing = np.argmax(label_missing_onehot, axis=2)\n",
        "          label_missing = test_dataset[0][i:i + 1, missing_index]\n",
        "          true_labels.extend(label_missing.ravel().tolist())\n",
        "          # accuracy\n",
        "          correct_prediction = np.equal(predict_missing_final, label_missing)\n",
        "          accuracy = np.mean(correct_prediction)\n",
        "          # print('{}/{}, accuracy: {:.4f}'.format(\n",
        "          #     i, test_X_missing.shape[0], accuracy))\n",
        "\n",
        "          avg_accuracy.append(accuracy)\n",
        "\n",
        "      # df = pd.DataFrame(to_save_array, columns= headers[:-1], index = test_indices)\n",
        "      # df.to_csv(save_name)\n",
        "      print('The average imputation accuracy' \\\n",
        "            'on test data with {} missing genotypes is {:.4f}: '\n",
        "          .format(missing_perc, np.mean(avg_accuracy)))\n",
        "      cnf_matrix = confusion_matrix(true_labels, preds)\n",
        "      FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
        "      FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
        "      TP = np.diag(cnf_matrix)\n",
        "      TN = cnf_matrix.sum() - (FP + FN + TP)\n",
        "      FP = FP.astype(float)\n",
        "      FN = FN.astype(float)\n",
        "      TP = TP.astype(float)\n",
        "      TN = TN.astype(float)\n",
        "      # Sensitivity, hit rate, recall, or true positive rate\n",
        "      TPR = TP/(TP+FN)\n",
        "      # Specificity or true negative rate\n",
        "      TNR = TN/(TN+FP)\n",
        "      print(f\"Sensitivity: {np.mean(TPR)}\")\n",
        "      print(f\"Specificity: {np.mean(TNR)}\")\n",
        "      print(f\"F1-score macro: {f1_score(true_labels, preds, average='macro')}\")\n",
        "      print(f\"F1-score micro: {f1_score(true_labels, preds, average='micro')}\")\n",
        "#     accuracies.append(np.mean(avg_accuracy))\n",
        "#     print(f\"=====================================\")\n",
        "# print('--------------------------------------------------------------------------')\n",
        "# print('--------------------------------------------------------------------------')\n",
        "# print('The overal average imputation accuracy over 10 runs' \\\n",
        "#         'on test data with {} missing genotypes is {:.4f}: '\n",
        "#       .format(missing_perc, np.mean(accuracies)))\n",
        "# print('--------------------------------------------------------------------------')\n",
        "# print(accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5fF1hLvj9Jb",
        "outputId": "4bb29051-8ea0-45ba-977c-4fd931e518c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training using seed 0\n",
            "x_train shape: (2003, 7161)\n",
            "x_test shape: (501, 7161)\n",
            "Epoch 1/1000\n",
            "100/100 - 532s - loss: 0.1934 - categorical_accuracy: 0.9354 - val_loss: 0.6411 - val_categorical_accuracy: 0.8139 - lr: 0.0100 - 532s/epoch - 5s/step\n",
            "Epoch 2/1000\n",
            "100/100 - 77s - loss: 0.0334 - categorical_accuracy: 0.9890 - val_loss: 0.4695 - val_categorical_accuracy: 0.8764 - lr: 0.0100 - 77s/epoch - 769ms/step\n",
            "Epoch 3/1000\n",
            "100/100 - 77s - loss: 0.0243 - categorical_accuracy: 0.9919 - val_loss: 0.2093 - val_categorical_accuracy: 0.9529 - lr: 0.0100 - 77s/epoch - 769ms/step\n",
            "Epoch 4/1000\n",
            "100/100 - 77s - loss: 0.0209 - categorical_accuracy: 0.9930 - val_loss: 0.0612 - val_categorical_accuracy: 0.9818 - lr: 0.0100 - 77s/epoch - 771ms/step\n",
            "Epoch 5/1000\n",
            "100/100 - 74s - loss: 0.0191 - categorical_accuracy: 0.9936 - val_loss: 0.3468 - val_categorical_accuracy: 0.9505 - lr: 0.0100 - 74s/epoch - 740ms/step\n",
            "Epoch 6/1000\n",
            "100/100 - 74s - loss: 0.0179 - categorical_accuracy: 0.9940 - val_loss: 1.6682 - val_categorical_accuracy: 0.8876 - lr: 0.0100 - 74s/epoch - 740ms/step\n",
            "Epoch 7/1000\n",
            "100/100 - 74s - loss: 0.0172 - categorical_accuracy: 0.9942 - val_loss: 1.3340 - val_categorical_accuracy: 0.8862 - lr: 0.0100 - 74s/epoch - 739ms/step\n",
            "Epoch 8/1000\n",
            "100/100 - 74s - loss: 0.0165 - categorical_accuracy: 0.9945 - val_loss: 0.1801 - val_categorical_accuracy: 0.9486 - lr: 0.0100 - 74s/epoch - 739ms/step\n",
            "Epoch 9/1000\n",
            "100/100 - 77s - loss: 0.0116 - categorical_accuracy: 0.9961 - val_loss: 0.0341 - val_categorical_accuracy: 0.9858 - lr: 0.0020 - 77s/epoch - 771ms/step\n",
            "Epoch 10/1000\n",
            "100/100 - 77s - loss: 0.0097 - categorical_accuracy: 0.9966 - val_loss: 0.0179 - val_categorical_accuracy: 0.9937 - lr: 0.0020 - 77s/epoch - 770ms/step\n",
            "Epoch 11/1000\n",
            "100/100 - 74s - loss: 0.0090 - categorical_accuracy: 0.9969 - val_loss: 0.0529 - val_categorical_accuracy: 0.9881 - lr: 0.0020 - 74s/epoch - 739ms/step\n",
            "Epoch 12/1000\n",
            "100/100 - 74s - loss: 0.0085 - categorical_accuracy: 0.9970 - val_loss: 0.0522 - val_categorical_accuracy: 0.9911 - lr: 0.0020 - 74s/epoch - 740ms/step\n",
            "Epoch 13/1000\n",
            "100/100 - 74s - loss: 0.0081 - categorical_accuracy: 0.9971 - val_loss: 0.0441 - val_categorical_accuracy: 0.9867 - lr: 0.0020 - 74s/epoch - 740ms/step\n",
            "Epoch 14/1000\n",
            "100/100 - 77s - loss: 0.0079 - categorical_accuracy: 0.9972 - val_loss: 0.0113 - val_categorical_accuracy: 0.9964 - lr: 0.0020 - 77s/epoch - 769ms/step\n",
            "Epoch 15/1000\n",
            "100/100 - 74s - loss: 0.0076 - categorical_accuracy: 0.9973 - val_loss: 0.0241 - val_categorical_accuracy: 0.9919 - lr: 0.0020 - 74s/epoch - 739ms/step\n",
            "Epoch 16/1000\n",
            "100/100 - 74s - loss: 0.0074 - categorical_accuracy: 0.9974 - val_loss: 0.0157 - val_categorical_accuracy: 0.9951 - lr: 0.0020 - 74s/epoch - 739ms/step\n",
            "Epoch 17/1000\n",
            "100/100 - 74s - loss: 0.0071 - categorical_accuracy: 0.9975 - val_loss: 0.0149 - val_categorical_accuracy: 0.9950 - lr: 0.0020 - 74s/epoch - 739ms/step\n",
            "Epoch 18/1000\n",
            "100/100 - 74s - loss: 0.0070 - categorical_accuracy: 0.9975 - val_loss: 0.0176 - val_categorical_accuracy: 0.9937 - lr: 0.0020 - 74s/epoch - 739ms/step\n",
            "Epoch 19/1000\n",
            "100/100 - 75s - loss: 0.0061 - categorical_accuracy: 0.9978 - val_loss: 0.0186 - val_categorical_accuracy: 0.9929 - lr: 4.0000e-04 - 75s/epoch - 751ms/step\n",
            "Epoch 20/1000\n",
            "100/100 - 77s - loss: 0.0056 - categorical_accuracy: 0.9980 - val_loss: 0.0107 - val_categorical_accuracy: 0.9968 - lr: 4.0000e-04 - 77s/epoch - 770ms/step\n",
            "Epoch 21/1000\n",
            "100/100 - 77s - loss: 0.0054 - categorical_accuracy: 0.9981 - val_loss: 0.0107 - val_categorical_accuracy: 0.9968 - lr: 4.0000e-04 - 77s/epoch - 769ms/step\n",
            "Epoch 22/1000\n",
            "100/100 - 74s - loss: 0.0053 - categorical_accuracy: 0.9981 - val_loss: 0.0107 - val_categorical_accuracy: 0.9968 - lr: 4.0000e-04 - 74s/epoch - 739ms/step\n",
            "Epoch 23/1000\n",
            "100/100 - 74s - loss: 0.0051 - categorical_accuracy: 0.9981 - val_loss: 0.0109 - val_categorical_accuracy: 0.9968 - lr: 4.0000e-04 - 74s/epoch - 740ms/step\n",
            "Epoch 24/1000\n",
            "100/100 - 74s - loss: 0.0050 - categorical_accuracy: 0.9982 - val_loss: 0.0110 - val_categorical_accuracy: 0.9968 - lr: 4.0000e-04 - 74s/epoch - 739ms/step\n",
            "Epoch 25/1000\n",
            "100/100 - 74s - loss: 0.0049 - categorical_accuracy: 0.9982 - val_loss: 0.0110 - val_categorical_accuracy: 0.9968 - lr: 8.0000e-05 - 74s/epoch - 740ms/step\n",
            "Epoch 26/1000\n",
            "100/100 - 74s - loss: 0.0048 - categorical_accuracy: 0.9982 - val_loss: 0.0111 - val_categorical_accuracy: 0.9968 - lr: 8.0000e-05 - 74s/epoch - 739ms/step\n",
            "Epoch 27/1000\n",
            "100/100 - 74s - loss: 0.0048 - categorical_accuracy: 0.9983 - val_loss: 0.0108 - val_categorical_accuracy: 0.9969 - lr: 8.0000e-05 - 74s/epoch - 739ms/step\n",
            "Epoch 28/1000\n",
            "100/100 - 74s - loss: 0.0046 - categorical_accuracy: 0.9983 - val_loss: 0.0108 - val_categorical_accuracy: 0.9969 - lr: 8.0000e-05 - 74s/epoch - 739ms/step\n",
            "Epoch 29/1000\n",
            "100/100 - 74s - loss: 0.0047 - categorical_accuracy: 0.9983 - val_loss: 0.0107 - val_categorical_accuracy: 0.9969 - lr: 1.6000e-05 - 74s/epoch - 739ms/step\n",
            "Epoch 30/1000\n",
            "100/100 - 74s - loss: 0.0046 - categorical_accuracy: 0.9983 - val_loss: 0.0108 - val_categorical_accuracy: 0.9969 - lr: 1.6000e-05 - 74s/epoch - 739ms/step\n",
            "Epoch 31/1000\n",
            "Restoring model weights from the end of the best epoch: 21.\n",
            "100/100 - 81s - loss: 0.0047 - categorical_accuracy: 0.9983 - val_loss: 0.0108 - val_categorical_accuracy: 0.9969 - lr: 1.6000e-05 - 81s/epoch - 809ms/step\n",
            "Epoch 31: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501/501 [06:59<00:00,  1.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average imputation accuracyon test data with 0.1 missing genotypes is 0.9907: \n",
            "Sensitivity: 0.9871519585377297\n",
            "Specificity: 0.9962728442892401\n",
            "F1-score macro: 0.9880433943247573\n",
            "F1-score micro: 0.9906918007560298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTy3WotVN74D"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# missing_perc = 0.1\n",
        "# model_size = 128\n",
        "# # N_SPLITS = 5\n",
        "# # batch_size = 50\n",
        "# accuracies = []\n",
        "# for random_state in range(0, 1000, 1000):\n",
        "#   for population_label in np.unique(Y_train.values)[:1]:\n",
        "    \n",
        "#     print(f\"Training using seed {random_state}\")\n",
        "#     # train_dataset, valid_dataset, test_dataset, steps_per_epoch, validation_steps = get_three_sets(X, attention_mask, Y_train, batch_size, population_label, missing_perc)\n",
        "#     x_train, y_train, test_dataset, test_indices = get_three_sets(X, Y_train, population_label, missing_perc)\n",
        "#     # kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=random_state)\n",
        "#     # for fold,(x_train_ids,x_valid_ids) in enumerate(kfold.split(x_train,y_train)):\n",
        "#     # print(f\"Fold: {fold}\")\n",
        "#     # print(x_train.shape)\n",
        "#     K.clear_session()\n",
        "#     # print(x_train_ids.shape, x_valid_ids.shape)\n",
        "#     # x_train_data = x_train[x_train_ids]\n",
        "#     # x_valid_data = x_train[x_valid_ids]\n",
        "#     # x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.20,\n",
        "#     #                                     random_state=random_state,\n",
        "#     #                                     shuffle=True,\n",
        "#     #                                     stratify=y_train)\n",
        "    \n",
        "#     num_epochs = 1000\n",
        "#     steps_per_epoch = 2*x_train.shape[0]//BATCH_SIZE\n",
        "#     # validation_steps = 2*x_valid.shape[0]//BATCH_SIZE\n",
        "#     # train_dataset = DataGenerator(x_train, batch_size, missing_perc=missing_perc)\n",
        "#     # train_dataset = get_dataset(x_train, 0, feature_size, 0, 0, batch_size, repeats=5)\n",
        "#     # train_dataset = get_dataset(x_train, 0, feature_size, 0, 0, BATCH_SIZE)\n",
        "#     train_dataset = get_dataset(x_train, 0, chunk_size+attention_range, 0, attention_range, BATCH_SIZE)\n",
        "#     # valid_dataset = get_dataset(x_valid, 0, chunk_size+attention_range, 0, attention_range, BATCH_SIZE, training=False)\n",
        "#     callbacks = create_callbacks()\n",
        "\n",
        "#     with strategy.scope():\n",
        "#       model = create_model(model_size)\n",
        "#       # model.build((BATCH_SIZE, feature_size, inChannel))\n",
        "#       model.build((BATCH_SIZE, chunk_size+attention_range, inChannel))\n",
        "#       optimizer = tfa.optimizers.LAMB(\n",
        "#             learning_rate=learning_rate\n",
        "#         )\n",
        "#       # optimizer = tfa.optimizers.LazyAdam(learning_rate=learning_rate)\n",
        "#       # model.compile(optimizer, steps_per_execution = steps_per_epoch//2)\n",
        "#       model.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=tf.keras.metrics.CategoricalAccuracy())\n",
        "\n",
        "#       history = model.fit(\n",
        "#           train_dataset,\n",
        "#           steps_per_epoch=steps_per_epoch,\n",
        "#           # validation_data=valid_dataset,\n",
        "#           # validation_steps=validation_steps,\n",
        "#           epochs=num_epochs,\n",
        "#           verbose=2,\n",
        "#           callbacks=callbacks\n",
        "#       )\n",
        "#     for missing_perc in [\n",
        "#                         #  0.01, 0.05,\n",
        "#                          0.1,\n",
        "#                         #  0.2\n",
        "#                          ]:\n",
        "#       save_name = f\"./drive/My Drive/ShiLab/HLA/mixed_pop/preds_mixed_mr_{missing_perc}_rs_{random_state}_.csv\"\n",
        "#       avg_accuracy = []\n",
        "#       preds = []\n",
        "#       true_labels = []\n",
        "      \n",
        "#       to_save_array = np.zeros((test_dataset[0].shape[0], test_dataset[0].shape[1]), dtype=object)\n",
        "#       test_X_missing = np.empty((test_dataset[0].shape[0] * 2, test_dataset[0].shape[1]), dtype=test_dataset[0].dtype)\n",
        "#       map_values_1_vec = np.vectorize(map_values_1)\n",
        "#       map_values_2_vec = np.vectorize(map_values_2)\n",
        "#       test_X_missing[0::2] = map_values_1_vec(test_dataset[0])\n",
        "#       test_X_missing[1::2] = map_values_2_vec(test_dataset[0])\n",
        "#       test_X_missing = to_categorical(test_X_missing, 3)\n",
        "#       for i in tqdm(range(test_dataset[0].shape[0])):\n",
        "#           rng = np.random.default_rng(i)\n",
        "#           # Generates missing genotypes\n",
        "#           missing_size = int(missing_perc * test_X_missing.shape[1])\n",
        "#           missing_index = rng.integers(low=0, high=test_X_missing.shape[1],\n",
        "#                                             size=missing_size)\n",
        "#           test_X_missing[i*2:i*2+2, missing_index, :] = [0, 0, 1]\n",
        "#           # mMask = np.tile(attention_mask, (1, 1, 1))\n",
        "#           # predict\n",
        "#           predict_onehot = model.predict(test_X_missing[i*2:i*2+2, :, :])\n",
        "#           # predict_onehot = model.predict([test_X_missing[i:i + 1, :, :], mMask])\n",
        "#           # only care the missing position\n",
        "#           predict_missing_onehot = predict_onehot[0:2, list(filter(lambda x: x<chunk_size, missing_index)), :]\n",
        "#           # predict label\n",
        "#           predict_missing = np.argmax(predict_missing_onehot, axis=2)\n",
        "#           predict_missing_final = np.zeros((1, predict_missing.shape[1]))\n",
        "#           for j in range(predict_missing.shape[1]):\n",
        "#             if predict_missing[:, j].tolist() == [0, 0]:\n",
        "#               predict_missing_final[:, j] = 0\n",
        "#             elif predict_missing[:, j].tolist() == [0, 1]:\n",
        "#               predict_missing_final[:, j] = 1\n",
        "#             elif predict_missing[:, j].tolist() == [1, 0]:\n",
        "#               predict_missing_final[:, j] = 2\n",
        "#             elif predict_missing[:, j].tolist() == [1, 1]:\n",
        "#               predict_missing_final[:, j] = 3\n",
        "#             else:\n",
        "#               predict_missing_final[:, j] = 4\n",
        "#           preds.extend(predict_missing_final.ravel().tolist())\n",
        "          \n",
        "#           predict_haplotypes = np.argmax(predict_onehot, axis=2)\n",
        "#           for j in range(predict_onehot.shape[1]):\n",
        "#             if predict_haplotypes[:, j].tolist() == [0,0]:\n",
        "#               to_save_array[i, j] = '0|0'\n",
        "#             elif predict_haplotypes[:, j].tolist() == [0,1]:\n",
        "#               to_save_array[i, j] = '0|1'\n",
        "#             elif predict_haplotypes[:, j].tolist() == [1,0]:\n",
        "#               to_save_array[i, j] = '1|0'\n",
        "#             elif predict_haplotypes[:, j].tolist() == [1, 1]:\n",
        "#               to_save_array[i, j] = '1|1'\n",
        "#             else:\n",
        "#               to_save_array[i, j] = '.|.'\n",
        "#           # real label\n",
        "#           # label_missing_onehot = test_dataset[0][i:i + 1, missing_index, :]\n",
        "#           # label_missing = np.argmax(label_missing_onehot, axis=2)\n",
        "#           label_missing = test_dataset[0][i:i + 1, list(filter(lambda x: x<chunk_size, missing_index))]\n",
        "#           true_labels.extend(label_missing.ravel().tolist())\n",
        "#           # accuracy\n",
        "#           correct_prediction = np.equal(predict_missing_final, label_missing)\n",
        "#           accuracy = np.mean(correct_prediction)\n",
        "#           # print('{}/{}, accuracy: {:.4f}'.format(\n",
        "#           #     i, test_X_missing.shape[0], accuracy))\n",
        "\n",
        "#           avg_accuracy.append(accuracy)\n",
        "\n",
        "#       # df = pd.DataFrame(to_save_array, columns= headers[:-1], index = test_indices)\n",
        "#       # df.to_csv(save_name)\n",
        "#       print('The average imputation accuracy' \\\n",
        "#             'on test data with {} missing genotypes is {:.4f}: '\n",
        "#           .format(missing_perc, np.mean(avg_accuracy)))\n",
        "#       cnf_matrix = confusion_matrix(true_labels, preds)\n",
        "#       FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
        "#       FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
        "#       TP = np.diag(cnf_matrix)\n",
        "#       TN = cnf_matrix.sum() - (FP + FN + TP)\n",
        "#       FP = FP.astype(float)\n",
        "#       FN = FN.astype(float)\n",
        "#       TP = TP.astype(float)\n",
        "#       TN = TN.astype(float)\n",
        "#       # Sensitivity, hit rate, recall, or true positive rate\n",
        "#       TPR = TP/(TP+FN)\n",
        "#       # Specificity or true negative rate\n",
        "#       TNR = TN/(TN+FP)\n",
        "#       print(f\"Sensitivity: {np.mean(TPR)}\")\n",
        "#       print(f\"Specificity: {np.mean(TNR)}\")\n",
        "#       print(f\"F1-score macro: {f1_score(true_labels, preds, average='macro')}\")\n",
        "#       print(f\"F1-score micro: {f1_score(true_labels, preds, average='micro')}\")\n",
        "# #     accuracies.append(np.mean(avg_accuracy))\n",
        "# #     print(f\"=====================================\")\n",
        "# # print('--------------------------------------------------------------------------')\n",
        "# # print('--------------------------------------------------------------------------')\n",
        "# # print('The overal average imputation accuracy over 10 runs' \\\n",
        "# #         'on test data with {} missing genotypes is {:.4f}: '\n",
        "# #       .format(missing_perc, np.mean(accuracies)))\n",
        "# # print('--------------------------------------------------------------------------')\n",
        "# # print(accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8zAjO1P2pO4",
        "outputId": "2d3e4142-ea9f-4aee-8098-b5520604dad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training using seed 0\n",
            "x_train shape: (2003, 7161)\n",
            "x_test shape: (501, 7161)\n",
            "Epoch 1/1000\n",
            "62/62 - 39s - loss: 0.1984 - categorical_accuracy: 0.9199 - lr: 0.0100 - 39s/epoch - 625ms/step\n",
            "Epoch 2/1000\n",
            "62/62 - 2s - loss: 0.0273 - categorical_accuracy: 0.9908 - lr: 0.0100 - 2s/epoch - 32ms/step\n",
            "Epoch 3/1000\n",
            "62/62 - 2s - loss: 0.0191 - categorical_accuracy: 0.9937 - lr: 0.0100 - 2s/epoch - 32ms/step\n",
            "Epoch 4/1000\n",
            "62/62 - 2s - loss: 0.0165 - categorical_accuracy: 0.9945 - lr: 0.0100 - 2s/epoch - 32ms/step\n",
            "Epoch 5/1000\n",
            "62/62 - 2s - loss: 0.0151 - categorical_accuracy: 0.9949 - lr: 0.0100 - 2s/epoch - 32ms/step\n",
            "Epoch 6/1000\n",
            "62/62 - 2s - loss: 0.0137 - categorical_accuracy: 0.9954 - lr: 0.0100 - 2s/epoch - 32ms/step\n",
            "Epoch 7/1000\n",
            "62/62 - 2s - loss: 0.0131 - categorical_accuracy: 0.9956 - lr: 0.0100 - 2s/epoch - 32ms/step\n",
            "Epoch 8/1000\n",
            "62/62 - 2s - loss: 0.0125 - categorical_accuracy: 0.9958 - lr: 0.0100 - 2s/epoch - 32ms/step\n",
            "Epoch 9/1000\n",
            "62/62 - 2s - loss: 0.0122 - categorical_accuracy: 0.9959 - lr: 0.0100 - 2s/epoch - 33ms/step\n",
            "Epoch 10/1000\n",
            "62/62 - 2s - loss: 0.0119 - categorical_accuracy: 0.9960 - lr: 0.0100 - 2s/epoch - 33ms/step\n",
            "Epoch 11/1000\n",
            "62/62 - 2s - loss: 0.0118 - categorical_accuracy: 0.9960 - lr: 0.0100 - 2s/epoch - 33ms/step\n",
            "Epoch 12/1000\n",
            "62/62 - 2s - loss: 0.0114 - categorical_accuracy: 0.9962 - lr: 0.0100 - 2s/epoch - 33ms/step\n",
            "Epoch 13/1000\n",
            "62/62 - 2s - loss: 0.0113 - categorical_accuracy: 0.9962 - lr: 0.0100 - 2s/epoch - 33ms/step\n",
            "Epoch 14/1000\n",
            "62/62 - 2s - loss: 0.0110 - categorical_accuracy: 0.9963 - lr: 0.0100 - 2s/epoch - 32ms/step\n",
            "Epoch 15/1000\n",
            "62/62 - 2s - loss: 0.0107 - categorical_accuracy: 0.9964 - lr: 0.0100 - 2s/epoch - 32ms/step\n",
            "Epoch 16/1000\n",
            "62/62 - 2s - loss: 0.0104 - categorical_accuracy: 0.9966 - lr: 0.0100 - 2s/epoch - 33ms/step\n",
            "Epoch 17/1000\n",
            "62/62 - 2s - loss: 0.0102 - categorical_accuracy: 0.9966 - lr: 0.0100 - 2s/epoch - 32ms/step\n",
            "Epoch 18/1000\n",
            "62/62 - 2s - loss: 0.0103 - categorical_accuracy: 0.9965 - lr: 0.0100 - 2s/epoch - 33ms/step\n",
            "Epoch 19/1000\n",
            "62/62 - 2s - loss: 0.0103 - categorical_accuracy: 0.9966 - lr: 0.0100 - 2s/epoch - 33ms/step\n",
            "Epoch 20/1000\n",
            "62/62 - 2s - loss: 0.0100 - categorical_accuracy: 0.9967 - lr: 0.0100 - 2s/epoch - 32ms/step\n",
            "Epoch 21/1000\n",
            "62/62 - 2s - loss: 0.0101 - categorical_accuracy: 0.9967 - lr: 0.0100 - 2s/epoch - 33ms/step\n",
            "Epoch 22/1000\n",
            "62/62 - 2s - loss: 0.0100 - categorical_accuracy: 0.9967 - lr: 0.0100 - 2s/epoch - 32ms/step\n",
            "Epoch 23/1000\n",
            "62/62 - 2s - loss: 0.0099 - categorical_accuracy: 0.9968 - lr: 0.0100 - 2s/epoch - 33ms/step\n",
            "Epoch 24/1000\n",
            "62/62 - 2s - loss: 0.0097 - categorical_accuracy: 0.9968 - lr: 0.0100 - 2s/epoch - 33ms/step\n",
            "Epoch 25/1000\n",
            "62/62 - 2s - loss: 0.0099 - categorical_accuracy: 0.9967 - lr: 0.0100 - 2s/epoch - 33ms/step\n",
            "Epoch 26/1000\n",
            "62/62 - 2s - loss: 0.0098 - categorical_accuracy: 0.9967 - lr: 0.0100 - 2s/epoch - 33ms/step\n",
            "Epoch 27/1000\n",
            "62/62 - 2s - loss: 0.0097 - categorical_accuracy: 0.9968 - lr: 0.0100 - 2s/epoch - 33ms/step\n",
            "Epoch 28/1000\n",
            "62/62 - 2s - loss: 0.0097 - categorical_accuracy: 0.9968 - lr: 0.0100 - 2s/epoch - 33ms/step\n",
            "Epoch 29/1000\n",
            "62/62 - 2s - loss: 0.0072 - categorical_accuracy: 0.9976 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 30/1000\n",
            "62/62 - 2s - loss: 0.0065 - categorical_accuracy: 0.9978 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 31/1000\n",
            "62/62 - 2s - loss: 0.0063 - categorical_accuracy: 0.9978 - lr: 0.0020 - 2s/epoch - 32ms/step\n",
            "Epoch 32/1000\n",
            "62/62 - 2s - loss: 0.0060 - categorical_accuracy: 0.9980 - lr: 0.0020 - 2s/epoch - 32ms/step\n",
            "Epoch 33/1000\n",
            "62/62 - 2s - loss: 0.0058 - categorical_accuracy: 0.9980 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 34/1000\n",
            "62/62 - 2s - loss: 0.0058 - categorical_accuracy: 0.9980 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 35/1000\n",
            "62/62 - 2s - loss: 0.0056 - categorical_accuracy: 0.9980 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 36/1000\n",
            "62/62 - 2s - loss: 0.0057 - categorical_accuracy: 0.9981 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 37/1000\n",
            "62/62 - 2s - loss: 0.0054 - categorical_accuracy: 0.9981 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 38/1000\n",
            "62/62 - 2s - loss: 0.0056 - categorical_accuracy: 0.9980 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 39/1000\n",
            "62/62 - 2s - loss: 0.0053 - categorical_accuracy: 0.9982 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 40/1000\n",
            "62/62 - 2s - loss: 0.0054 - categorical_accuracy: 0.9982 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 41/1000\n",
            "62/62 - 2s - loss: 0.0052 - categorical_accuracy: 0.9982 - lr: 0.0020 - 2s/epoch - 32ms/step\n",
            "Epoch 42/1000\n",
            "62/62 - 2s - loss: 0.0052 - categorical_accuracy: 0.9982 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 43/1000\n",
            "62/62 - 2s - loss: 0.0051 - categorical_accuracy: 0.9983 - lr: 0.0020 - 2s/epoch - 32ms/step\n",
            "Epoch 44/1000\n",
            "62/62 - 2s - loss: 0.0052 - categorical_accuracy: 0.9982 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 45/1000\n",
            "62/62 - 2s - loss: 0.0052 - categorical_accuracy: 0.9982 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 46/1000\n",
            "62/62 - 2s - loss: 0.0051 - categorical_accuracy: 0.9983 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 47/1000\n",
            "62/62 - 2s - loss: 0.0049 - categorical_accuracy: 0.9983 - lr: 0.0020 - 2s/epoch - 32ms/step\n",
            "Epoch 48/1000\n",
            "62/62 - 2s - loss: 0.0050 - categorical_accuracy: 0.9982 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 49/1000\n",
            "62/62 - 2s - loss: 0.0050 - categorical_accuracy: 0.9983 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 50/1000\n",
            "62/62 - 2s - loss: 0.0047 - categorical_accuracy: 0.9983 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 51/1000\n",
            "62/62 - 2s - loss: 0.0049 - categorical_accuracy: 0.9983 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 52/1000\n",
            "62/62 - 2s - loss: 0.0049 - categorical_accuracy: 0.9983 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 53/1000\n",
            "62/62 - 2s - loss: 0.0049 - categorical_accuracy: 0.9983 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 54/1000\n",
            "62/62 - 2s - loss: 0.0047 - categorical_accuracy: 0.9983 - lr: 0.0020 - 2s/epoch - 33ms/step\n",
            "Epoch 55/1000\n",
            "62/62 - 2s - loss: 0.0044 - categorical_accuracy: 0.9984 - lr: 4.0000e-04 - 2s/epoch - 33ms/step\n",
            "Epoch 56/1000\n",
            "62/62 - 2s - loss: 0.0042 - categorical_accuracy: 0.9985 - lr: 4.0000e-04 - 2s/epoch - 33ms/step\n",
            "Epoch 57/1000\n",
            "62/62 - 2s - loss: 0.0042 - categorical_accuracy: 0.9985 - lr: 4.0000e-04 - 2s/epoch - 33ms/step\n",
            "Epoch 58/1000\n",
            "62/62 - 2s - loss: 0.0041 - categorical_accuracy: 0.9985 - lr: 4.0000e-04 - 2s/epoch - 33ms/step\n",
            "Epoch 59/1000\n",
            "62/62 - 2s - loss: 0.0041 - categorical_accuracy: 0.9985 - lr: 4.0000e-04 - 2s/epoch - 32ms/step\n",
            "Epoch 60/1000\n",
            "62/62 - 2s - loss: 0.0040 - categorical_accuracy: 0.9986 - lr: 4.0000e-04 - 2s/epoch - 33ms/step\n",
            "Epoch 61/1000\n",
            "62/62 - 2s - loss: 0.0040 - categorical_accuracy: 0.9986 - lr: 4.0000e-04 - 2s/epoch - 33ms/step\n",
            "Epoch 62/1000\n",
            "62/62 - 2s - loss: 0.0040 - categorical_accuracy: 0.9986 - lr: 4.0000e-04 - 2s/epoch - 33ms/step\n",
            "Epoch 63/1000\n",
            "62/62 - 2s - loss: 0.0039 - categorical_accuracy: 0.9986 - lr: 8.0000e-05 - 2s/epoch - 33ms/step\n",
            "Epoch 64/1000\n",
            "62/62 - 2s - loss: 0.0039 - categorical_accuracy: 0.9986 - lr: 8.0000e-05 - 2s/epoch - 33ms/step\n",
            "Epoch 65/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9986 - lr: 8.0000e-05 - 2s/epoch - 33ms/step\n",
            "Epoch 66/1000\n",
            "62/62 - 2s - loss: 0.0039 - categorical_accuracy: 0.9986 - lr: 8.0000e-05 - 2s/epoch - 33ms/step\n",
            "Epoch 67/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9986 - lr: 8.0000e-05 - 2s/epoch - 33ms/step\n",
            "Epoch 68/1000\n",
            "62/62 - 2s - loss: 0.0039 - categorical_accuracy: 0.9986 - lr: 1.6000e-05 - 2s/epoch - 33ms/step\n",
            "Epoch 69/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9986 - lr: 1.6000e-05 - 2s/epoch - 33ms/step\n",
            "Epoch 70/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9986 - lr: 1.6000e-05 - 2s/epoch - 33ms/step\n",
            "Epoch 71/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9986 - lr: 1.6000e-05 - 2s/epoch - 33ms/step\n",
            "Epoch 72/1000\n",
            "62/62 - 2s - loss: 0.0039 - categorical_accuracy: 0.9986 - lr: 3.2000e-06 - 2s/epoch - 33ms/step\n",
            "Epoch 73/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9986 - lr: 3.2000e-06 - 2s/epoch - 33ms/step\n",
            "Epoch 74/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9986 - lr: 3.2000e-06 - 2s/epoch - 32ms/step\n",
            "Epoch 75/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9986 - lr: 3.2000e-06 - 2s/epoch - 33ms/step\n",
            "Epoch 76/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9986 - lr: 6.4000e-07 - 2s/epoch - 32ms/step\n",
            "Epoch 77/1000\n",
            "62/62 - 2s - loss: 0.0039 - categorical_accuracy: 0.9986 - lr: 6.4000e-07 - 2s/epoch - 32ms/step\n",
            "Epoch 78/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9986 - lr: 6.4000e-07 - 2s/epoch - 33ms/step\n",
            "Epoch 79/1000\n",
            "62/62 - 2s - loss: 0.0040 - categorical_accuracy: 0.9986 - lr: 6.4000e-07 - 2s/epoch - 33ms/step\n",
            "Epoch 80/1000\n",
            "62/62 - 2s - loss: 0.0039 - categorical_accuracy: 0.9986 - lr: 1.2800e-07 - 2s/epoch - 32ms/step\n",
            "Epoch 81/1000\n",
            "62/62 - 2s - loss: 0.0039 - categorical_accuracy: 0.9986 - lr: 1.2800e-07 - 2s/epoch - 32ms/step\n",
            "Epoch 82/1000\n",
            "62/62 - 2s - loss: 0.0039 - categorical_accuracy: 0.9986 - lr: 1.2800e-07 - 2s/epoch - 32ms/step\n",
            "Epoch 83/1000\n",
            "62/62 - 2s - loss: 0.0039 - categorical_accuracy: 0.9986 - lr: 1.2800e-07 - 2s/epoch - 33ms/step\n",
            "Epoch 84/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9986 - lr: 2.5600e-08 - 2s/epoch - 32ms/step\n",
            "Epoch 85/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9986 - lr: 2.5600e-08 - 2s/epoch - 33ms/step\n",
            "Epoch 86/1000\n",
            "62/62 - 2s - loss: 0.0039 - categorical_accuracy: 0.9986 - lr: 2.5600e-08 - 2s/epoch - 33ms/step\n",
            "Epoch 87/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9987 - lr: 2.5600e-08 - 2s/epoch - 33ms/step\n",
            "Epoch 88/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9986 - lr: 2.5600e-08 - 2s/epoch - 33ms/step\n",
            "Epoch 89/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9987 - lr: 2.5600e-08 - 2s/epoch - 32ms/step\n",
            "Epoch 90/1000\n",
            "62/62 - 2s - loss: 0.0037 - categorical_accuracy: 0.9987 - lr: 2.5600e-08 - 2s/epoch - 33ms/step\n",
            "Epoch 91/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9987 - lr: 2.5600e-08 - 2s/epoch - 33ms/step\n",
            "Epoch 92/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9986 - lr: 5.1200e-09 - 2s/epoch - 33ms/step\n",
            "Epoch 93/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9986 - lr: 5.1200e-09 - 2s/epoch - 33ms/step\n",
            "Epoch 94/1000\n",
            "62/62 - 2s - loss: 0.0039 - categorical_accuracy: 0.9986 - lr: 5.1200e-09 - 2s/epoch - 33ms/step\n",
            "Epoch 95/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9986 - lr: 5.1200e-09 - 2s/epoch - 33ms/step\n",
            "Epoch 96/1000\n",
            "62/62 - 2s - loss: 0.0039 - categorical_accuracy: 0.9986 - lr: 1.0240e-09 - 2s/epoch - 33ms/step\n",
            "Epoch 97/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9986 - lr: 1.0240e-09 - 2s/epoch - 33ms/step\n",
            "Epoch 98/1000\n",
            "62/62 - 2s - loss: 0.0040 - categorical_accuracy: 0.9986 - lr: 1.0240e-09 - 2s/epoch - 33ms/step\n",
            "Epoch 99/1000\n",
            "62/62 - 2s - loss: 0.0038 - categorical_accuracy: 0.9987 - lr: 1.0240e-09 - 2s/epoch - 33ms/step\n",
            "Epoch 100/1000\n",
            "62/62 - 2s - loss: 0.0039 - categorical_accuracy: 0.9986 - lr: 2.0480e-10 - 2s/epoch - 33ms/step\n",
            "Epoch 100: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501/501 [04:29<00:00,  1.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average imputation accuracyon test data with 0.1 missing genotypes is 0.9960: \n",
            "Sensitivity: 0.994683726928269\n",
            "Specificity: 0.9985568862564768\n",
            "F1-score macro: 0.9947360546996551\n",
            "F1-score micro: 0.9960585022308477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCPjffKL_FXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9876099-871f-4841-ac5e-472718b1f5b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training using seed 0\n",
            "x_train shape: (2003, 7161)\n",
            "x_test shape: (501, 7161)\n",
            "Epoch 1/1000\n",
            "62/62 - 350s - loss: 0.1982 - categorical_accuracy: 0.9221 - lr: 0.0100 - 350s/epoch - 6s/step\n",
            "Epoch 2/1000\n",
            "62/62 - 15s - loss: 0.0341 - categorical_accuracy: 0.9883 - lr: 0.0100 - 15s/epoch - 248ms/step\n",
            "Epoch 3/1000\n",
            "62/62 - 15s - loss: 0.0253 - categorical_accuracy: 0.9913 - lr: 0.0100 - 15s/epoch - 250ms/step\n",
            "Epoch 4/1000\n",
            "62/62 - 16s - loss: 0.0222 - categorical_accuracy: 0.9924 - lr: 0.0100 - 16s/epoch - 250ms/step\n",
            "Epoch 5/1000\n",
            "62/62 - 15s - loss: 0.0204 - categorical_accuracy: 0.9930 - lr: 0.0100 - 15s/epoch - 248ms/step\n",
            "Epoch 6/1000\n",
            "62/62 - 15s - loss: 0.0193 - categorical_accuracy: 0.9934 - lr: 0.0100 - 15s/epoch - 249ms/step\n",
            "Epoch 7/1000\n",
            "62/62 - 15s - loss: 0.0185 - categorical_accuracy: 0.9937 - lr: 0.0100 - 15s/epoch - 249ms/step\n",
            "Epoch 8/1000\n",
            "62/62 - 15s - loss: 0.0177 - categorical_accuracy: 0.9939 - lr: 0.0100 - 15s/epoch - 249ms/step\n",
            "Epoch 9/1000\n",
            "62/62 - 15s - loss: 0.0173 - categorical_accuracy: 0.9941 - lr: 0.0100 - 15s/epoch - 249ms/step\n",
            "Epoch 10/1000\n",
            "62/62 - 15s - loss: 0.0169 - categorical_accuracy: 0.9942 - lr: 0.0100 - 15s/epoch - 248ms/step\n",
            "Epoch 11/1000\n",
            "62/62 - 15s - loss: 0.0166 - categorical_accuracy: 0.9943 - lr: 0.0100 - 15s/epoch - 248ms/step\n",
            "Epoch 12/1000\n",
            "62/62 - 16s - loss: 0.0161 - categorical_accuracy: 0.9945 - lr: 0.0100 - 16s/epoch - 250ms/step\n",
            "Epoch 13/1000\n",
            "62/62 - 15s - loss: 0.0158 - categorical_accuracy: 0.9946 - lr: 0.0100 - 15s/epoch - 248ms/step\n",
            "Epoch 14/1000\n",
            "62/62 - 15s - loss: 0.0157 - categorical_accuracy: 0.9946 - lr: 0.0100 - 15s/epoch - 250ms/step\n",
            "Epoch 15/1000\n",
            "62/62 - 15s - loss: 0.0156 - categorical_accuracy: 0.9947 - lr: 0.0100 - 15s/epoch - 249ms/step\n",
            "Epoch 16/1000\n",
            "62/62 - 15s - loss: 0.0152 - categorical_accuracy: 0.9948 - lr: 0.0100 - 15s/epoch - 249ms/step\n",
            "Epoch 17/1000\n",
            "62/62 - 15s - loss: 0.0152 - categorical_accuracy: 0.9948 - lr: 0.0100 - 15s/epoch - 248ms/step\n",
            "Epoch 18/1000\n",
            "62/62 - 15s - loss: 0.0151 - categorical_accuracy: 0.9949 - lr: 0.0100 - 15s/epoch - 250ms/step\n",
            "Epoch 19/1000\n",
            "62/62 - 15s - loss: 0.0150 - categorical_accuracy: 0.9949 - lr: 0.0100 - 15s/epoch - 249ms/step\n",
            "Epoch 20/1000\n",
            "62/62 - 15s - loss: 0.0149 - categorical_accuracy: 0.9949 - lr: 0.0100 - 15s/epoch - 248ms/step\n",
            "Epoch 21/1000\n",
            "62/62 - 15s - loss: 0.0147 - categorical_accuracy: 0.9950 - lr: 0.0100 - 15s/epoch - 249ms/step\n",
            "Epoch 22/1000\n",
            "62/62 - 16s - loss: 0.0148 - categorical_accuracy: 0.9950 - lr: 0.0100 - 16s/epoch - 250ms/step\n",
            "Epoch 23/1000\n",
            "62/62 - 15s - loss: 0.0146 - categorical_accuracy: 0.9950 - lr: 0.0100 - 15s/epoch - 250ms/step\n",
            "Epoch 24/1000\n",
            "62/62 - 15s - loss: 0.0147 - categorical_accuracy: 0.9950 - lr: 0.0100 - 15s/epoch - 249ms/step\n",
            "Epoch 25/1000\n",
            "62/62 - 15s - loss: 0.0145 - categorical_accuracy: 0.9951 - lr: 0.0100 - 15s/epoch - 250ms/step\n",
            "Epoch 26/1000\n",
            "62/62 - 15s - loss: 0.0145 - categorical_accuracy: 0.9951 - lr: 0.0100 - 15s/epoch - 249ms/step\n",
            "Epoch 27/1000\n",
            "62/62 - 15s - loss: 0.0144 - categorical_accuracy: 0.9951 - lr: 0.0100 - 15s/epoch - 250ms/step\n",
            "Epoch 28/1000\n",
            "62/62 - 15s - loss: 0.0144 - categorical_accuracy: 0.9951 - lr: 0.0100 - 15s/epoch - 249ms/step\n",
            "Epoch 29/1000\n",
            "62/62 - 15s - loss: 0.0145 - categorical_accuracy: 0.9951 - lr: 0.0100 - 15s/epoch - 249ms/step\n",
            "Epoch 30/1000\n",
            "62/62 - 15s - loss: 0.0116 - categorical_accuracy: 0.9960 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 31/1000\n",
            "62/62 - 15s - loss: 0.0104 - categorical_accuracy: 0.9964 - lr: 0.0020 - 15s/epoch - 250ms/step\n",
            "Epoch 32/1000\n",
            "62/62 - 15s - loss: 0.0101 - categorical_accuracy: 0.9965 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 33/1000\n",
            "62/62 - 15s - loss: 0.0098 - categorical_accuracy: 0.9966 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 34/1000\n",
            "62/62 - 15s - loss: 0.0096 - categorical_accuracy: 0.9967 - lr: 0.0020 - 15s/epoch - 248ms/step\n",
            "Epoch 35/1000\n",
            "62/62 - 15s - loss: 0.0095 - categorical_accuracy: 0.9967 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 36/1000\n",
            "62/62 - 15s - loss: 0.0093 - categorical_accuracy: 0.9967 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 37/1000\n",
            "62/62 - 15s - loss: 0.0092 - categorical_accuracy: 0.9968 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 38/1000\n",
            "62/62 - 15s - loss: 0.0091 - categorical_accuracy: 0.9968 - lr: 0.0020 - 15s/epoch - 250ms/step\n",
            "Epoch 39/1000\n",
            "62/62 - 15s - loss: 0.0090 - categorical_accuracy: 0.9968 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 40/1000\n",
            "62/62 - 15s - loss: 0.0089 - categorical_accuracy: 0.9969 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 41/1000\n",
            "62/62 - 15s - loss: 0.0088 - categorical_accuracy: 0.9969 - lr: 0.0020 - 15s/epoch - 250ms/step\n",
            "Epoch 42/1000\n",
            "62/62 - 15s - loss: 0.0088 - categorical_accuracy: 0.9969 - lr: 0.0020 - 15s/epoch - 248ms/step\n",
            "Epoch 43/1000\n",
            "62/62 - 15s - loss: 0.0087 - categorical_accuracy: 0.9970 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 44/1000\n",
            "62/62 - 15s - loss: 0.0087 - categorical_accuracy: 0.9969 - lr: 0.0020 - 15s/epoch - 250ms/step\n",
            "Epoch 45/1000\n",
            "62/62 - 15s - loss: 0.0085 - categorical_accuracy: 0.9970 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 46/1000\n",
            "62/62 - 15s - loss: 0.0085 - categorical_accuracy: 0.9970 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 47/1000\n",
            "62/62 - 15s - loss: 0.0085 - categorical_accuracy: 0.9970 - lr: 0.0020 - 15s/epoch - 248ms/step\n",
            "Epoch 48/1000\n",
            "62/62 - 15s - loss: 0.0085 - categorical_accuracy: 0.9970 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 49/1000\n",
            "62/62 - 15s - loss: 0.0084 - categorical_accuracy: 0.9971 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 50/1000\n",
            "62/62 - 15s - loss: 0.0083 - categorical_accuracy: 0.9971 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 51/1000\n",
            "62/62 - 15s - loss: 0.0083 - categorical_accuracy: 0.9971 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 52/1000\n",
            "62/62 - 15s - loss: 0.0082 - categorical_accuracy: 0.9971 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 53/1000\n",
            "62/62 - 15s - loss: 0.0082 - categorical_accuracy: 0.9971 - lr: 0.0020 - 15s/epoch - 248ms/step\n",
            "Epoch 54/1000\n",
            "62/62 - 15s - loss: 0.0081 - categorical_accuracy: 0.9971 - lr: 0.0020 - 15s/epoch - 250ms/step\n",
            "Epoch 55/1000\n",
            "62/62 - 15s - loss: 0.0082 - categorical_accuracy: 0.9971 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 56/1000\n",
            "62/62 - 15s - loss: 0.0081 - categorical_accuracy: 0.9972 - lr: 0.0020 - 15s/epoch - 250ms/step\n",
            "Epoch 57/1000\n",
            "62/62 - 15s - loss: 0.0080 - categorical_accuracy: 0.9972 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 58/1000\n",
            "62/62 - 15s - loss: 0.0080 - categorical_accuracy: 0.9972 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 59/1000\n",
            "62/62 - 15s - loss: 0.0080 - categorical_accuracy: 0.9972 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 60/1000\n",
            "62/62 - 15s - loss: 0.0080 - categorical_accuracy: 0.9972 - lr: 0.0020 - 15s/epoch - 248ms/step\n",
            "Epoch 61/1000\n",
            "62/62 - 15s - loss: 0.0079 - categorical_accuracy: 0.9972 - lr: 0.0020 - 15s/epoch - 249ms/step\n",
            "Epoch 62/1000\n",
            "62/62 - 15s - loss: 0.0079 - categorical_accuracy: 0.9972 - lr: 0.0020 - 15s/epoch - 250ms/step\n",
            "Epoch 63/1000\n",
            "62/62 - 15s - loss: 0.0074 - categorical_accuracy: 0.9974 - lr: 4.0000e-04 - 15s/epoch - 249ms/step\n",
            "Epoch 64/1000\n",
            "62/62 - 15s - loss: 0.0072 - categorical_accuracy: 0.9974 - lr: 4.0000e-04 - 15s/epoch - 249ms/step\n",
            "Epoch 65/1000\n",
            "62/62 - 15s - loss: 0.0070 - categorical_accuracy: 0.9975 - lr: 4.0000e-04 - 15s/epoch - 250ms/step\n",
            "Epoch 66/1000\n",
            "62/62 - 15s - loss: 0.0070 - categorical_accuracy: 0.9975 - lr: 4.0000e-04 - 15s/epoch - 249ms/step\n",
            "Epoch 67/1000\n",
            "62/62 - 15s - loss: 0.0069 - categorical_accuracy: 0.9975 - lr: 4.0000e-04 - 15s/epoch - 249ms/step\n",
            "Epoch 68/1000\n",
            "62/62 - 15s - loss: 0.0069 - categorical_accuracy: 0.9975 - lr: 4.0000e-04 - 15s/epoch - 248ms/step\n",
            "Epoch 69/1000\n",
            "62/62 - 15s - loss: 0.0068 - categorical_accuracy: 0.9975 - lr: 4.0000e-04 - 15s/epoch - 249ms/step\n",
            "Epoch 70/1000\n",
            "62/62 - 15s - loss: 0.0067 - categorical_accuracy: 0.9976 - lr: 4.0000e-04 - 15s/epoch - 250ms/step\n",
            "Epoch 71/1000\n",
            "62/62 - 15s - loss: 0.0068 - categorical_accuracy: 0.9976 - lr: 4.0000e-04 - 15s/epoch - 249ms/step\n",
            "Epoch 72/1000\n",
            "62/62 - 15s - loss: 0.0067 - categorical_accuracy: 0.9976 - lr: 4.0000e-04 - 15s/epoch - 249ms/step\n",
            "Epoch 73/1000\n",
            "62/62 - 15s - loss: 0.0067 - categorical_accuracy: 0.9976 - lr: 4.0000e-04 - 15s/epoch - 249ms/step\n",
            "Epoch 74/1000\n",
            "62/62 - 15s - loss: 0.0066 - categorical_accuracy: 0.9976 - lr: 4.0000e-04 - 15s/epoch - 249ms/step\n",
            "Epoch 75/1000\n",
            "62/62 - 15s - loss: 0.0066 - categorical_accuracy: 0.9976 - lr: 8.0000e-05 - 15s/epoch - 249ms/step\n",
            "Epoch 76/1000\n",
            "62/62 - 15s - loss: 0.0065 - categorical_accuracy: 0.9976 - lr: 8.0000e-05 - 15s/epoch - 249ms/step\n",
            "Epoch 77/1000\n",
            "62/62 - 15s - loss: 0.0066 - categorical_accuracy: 0.9976 - lr: 8.0000e-05 - 15s/epoch - 250ms/step\n",
            "Epoch 78/1000\n",
            "62/62 - 15s - loss: 0.0066 - categorical_accuracy: 0.9976 - lr: 8.0000e-05 - 15s/epoch - 249ms/step\n",
            "Epoch 79/1000\n",
            "62/62 - 15s - loss: 0.0065 - categorical_accuracy: 0.9977 - lr: 8.0000e-05 - 15s/epoch - 249ms/step\n",
            "Epoch 80/1000\n",
            "62/62 - 15s - loss: 0.0065 - categorical_accuracy: 0.9976 - lr: 8.0000e-05 - 15s/epoch - 250ms/step\n",
            "Epoch 81/1000\n",
            "62/62 - 15s - loss: 0.0065 - categorical_accuracy: 0.9976 - lr: 8.0000e-05 - 15s/epoch - 249ms/step\n",
            "Epoch 82/1000\n",
            "62/62 - 15s - loss: 0.0064 - categorical_accuracy: 0.9977 - lr: 8.0000e-05 - 15s/epoch - 249ms/step\n",
            "Epoch 83/1000\n",
            "62/62 - 15s - loss: 0.0065 - categorical_accuracy: 0.9977 - lr: 8.0000e-05 - 15s/epoch - 249ms/step\n",
            "Epoch 84/1000\n",
            "62/62 - 15s - loss: 0.0065 - categorical_accuracy: 0.9977 - lr: 1.6000e-05 - 15s/epoch - 250ms/step\n",
            "Epoch 85/1000\n",
            "62/62 - 15s - loss: 0.0064 - categorical_accuracy: 0.9977 - lr: 1.6000e-05 - 15s/epoch - 249ms/step\n",
            "Epoch 86/1000\n",
            "62/62 - 15s - loss: 0.0065 - categorical_accuracy: 0.9977 - lr: 1.6000e-05 - 15s/epoch - 249ms/step\n",
            "Epoch 87/1000\n",
            "62/62 - 15s - loss: 0.0065 - categorical_accuracy: 0.9977 - lr: 1.6000e-05 - 15s/epoch - 249ms/step\n",
            "Epoch 88/1000\n",
            "62/62 - 15s - loss: 0.0064 - categorical_accuracy: 0.9977 - lr: 3.2000e-06 - 15s/epoch - 249ms/step\n",
            "Epoch 89/1000\n",
            "62/62 - 15s - loss: 0.0064 - categorical_accuracy: 0.9977 - lr: 3.2000e-06 - 15s/epoch - 249ms/step\n",
            "Epoch 90/1000\n",
            "62/62 - 15s - loss: 0.0064 - categorical_accuracy: 0.9977 - lr: 3.2000e-06 - 15s/epoch - 249ms/step\n",
            "Epoch 91/1000\n",
            "62/62 - 15s - loss: 0.0064 - categorical_accuracy: 0.9977 - lr: 3.2000e-06 - 15s/epoch - 250ms/step\n",
            "Epoch 92/1000\n",
            "62/62 - 15s - loss: 0.0064 - categorical_accuracy: 0.9977 - lr: 6.4000e-07 - 15s/epoch - 249ms/step\n",
            "Epoch 93/1000\n",
            "62/62 - 15s - loss: 0.0064 - categorical_accuracy: 0.9977 - lr: 6.4000e-07 - 15s/epoch - 250ms/step\n",
            "Epoch 94/1000\n",
            "62/62 - 15s - loss: 0.0064 - categorical_accuracy: 0.9977 - lr: 6.4000e-07 - 15s/epoch - 249ms/step\n",
            "Epoch 95/1000\n",
            "62/62 - 15s - loss: 0.0064 - categorical_accuracy: 0.9977 - lr: 6.4000e-07 - 15s/epoch - 249ms/step\n",
            "Epoch 96/1000\n",
            "62/62 - 15s - loss: 0.0064 - categorical_accuracy: 0.9977 - lr: 1.2800e-07 - 15s/epoch - 250ms/step\n",
            "Epoch 97/1000\n",
            "62/62 - 15s - loss: 0.0065 - categorical_accuracy: 0.9977 - lr: 1.2800e-07 - 15s/epoch - 250ms/step\n",
            "Epoch 98/1000\n",
            "62/62 - 15s - loss: 0.0064 - categorical_accuracy: 0.9977 - lr: 1.2800e-07 - 15s/epoch - 250ms/step\n",
            "Epoch 99/1000\n",
            "62/62 - 15s - loss: 0.0064 - categorical_accuracy: 0.9977 - lr: 1.2800e-07 - 15s/epoch - 249ms/step\n",
            "Epoch 99: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501/501 [05:11<00:00,  1.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average imputation accuracyon test data with 0.01 missing genotypes is 0.9924: \n",
            "Sensitivity: 0.9898052778222948\n",
            "Specificity: 0.9970328602914843\n",
            "F1-score macro: 0.9902847212314898\n",
            "F1-score micro: 0.9924095471029771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501/501 [04:43<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average imputation accuracyon test data with 0.05 missing genotypes is 0.9924: \n",
            "Sensitivity: 0.9896660597486041\n",
            "Specificity: 0.9970077904911343\n",
            "F1-score macro: 0.990181864994617\n",
            "F1-score micro: 0.9923783717481238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501/501 [04:41<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average imputation accuracyon test data with 0.1 missing genotypes is 0.9922: \n",
            "Sensitivity: 0.9893789412541283\n",
            "Specificity: 0.9969468043182133\n",
            "F1-score macro: 0.9899540891576668\n",
            "F1-score micro: 0.9922250471124789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501/501 [04:42<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average imputation accuracyon test data with 0.2 missing genotypes is 0.9917: \n",
            "Sensitivity: 0.9885360379968184\n",
            "Specificity: 0.9967338311866989\n",
            "F1-score macro: 0.9892273552512331\n",
            "F1-score micro: 0.991695380189342\n"
          ]
        }
      ],
      "source": [
        "## embed_dim=128\n",
        "\n",
        "# missing_perc = 0.1\n",
        "# model_size = 128\n",
        "# # N_SPLITS = 5\n",
        "# # batch_size = 50\n",
        "# accuracies = []\n",
        "# for random_state in range(0, 1000, 1000):\n",
        "#   for population_label in np.unique(Y_train.values)[:1]:\n",
        "    \n",
        "#     print(f\"Training using seed {random_state}\")\n",
        "#     # train_dataset, valid_dataset, test_dataset, steps_per_epoch, validation_steps = get_three_sets(X, attention_mask, Y_train, batch_size, population_label, missing_perc)\n",
        "#     x_train, y_train, test_dataset, test_indices = get_three_sets(X, Y_train, population_label, missing_perc)\n",
        "#     # kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=random_state)\n",
        "#     # for fold,(x_train_ids,x_valid_ids) in enumerate(kfold.split(x_train,y_train)):\n",
        "#     # print(f\"Fold: {fold}\")\n",
        "#     # print(x_train.shape)\n",
        "#     K.clear_session()\n",
        "#     # print(x_train_ids.shape, x_valid_ids.shape)\n",
        "#     # x_train_data = x_train[x_train_ids]\n",
        "#     # x_valid_data = x_train[x_valid_ids]\n",
        "#     # x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.20,\n",
        "#     #                                     random_state=random_state,\n",
        "#     #                                     shuffle=True,\n",
        "#     #                                     stratify=y_train)\n",
        "    \n",
        "#     num_epochs = 1000\n",
        "#     steps_per_epoch = 2*x_train.shape[0]//BATCH_SIZE\n",
        "#     # validation_steps = 2*x_valid.shape[0]//BATCH_SIZE\n",
        "#     # train_dataset = DataGenerator(x_train, batch_size, missing_perc=missing_perc)\n",
        "#     # train_dataset = get_dataset(x_train, 0, feature_size, 0, 0, batch_size, repeats=5)\n",
        "#     train_dataset = get_dataset(x_train, 0, feature_size, 0, 0, BATCH_SIZE)\n",
        "#     # train_dataset = get_dataset(x_train, 0, chunk_size+attention_range, 0, attention_range, BATCH_SIZE)\n",
        "#     # valid_dataset = get_dataset(x_valid, 0, chunk_size+attention_range, 0, attention_range, BATCH_SIZE, training=False)\n",
        "#     callbacks = create_callbacks()\n",
        "\n",
        "#     with strategy.scope():\n",
        "#       model = create_model(model_size)\n",
        "#       model.build((BATCH_SIZE, feature_size, inChannel))\n",
        "#       optimizer = tfa.optimizers.LAMB(\n",
        "#             learning_rate=learning_rate\n",
        "#         )\n",
        "#       # optimizer = tfa.optimizers.LazyAdam(learning_rate=learning_rate)\n",
        "#       # model.compile(optimizer, steps_per_execution = steps_per_epoch//2)\n",
        "#       model.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=tf.keras.metrics.CategoricalAccuracy())\n",
        "\n",
        "#       history = model.fit(\n",
        "#           train_dataset,\n",
        "#           steps_per_epoch=steps_per_epoch,\n",
        "#           # validation_data=valid_dataset,\n",
        "#           # validation_steps=validation_steps,\n",
        "#           epochs=num_epochs,\n",
        "#           verbose=2,\n",
        "#           callbacks=callbacks\n",
        "#       )\n",
        "#     for missing_perc in [0.01, 0.05, 0.1, 0.2]:\n",
        "#       save_name = f\"./drive/My Drive/ShiLab/HLA/mixed_pop/preds_mixed_mr_{missing_perc}_rs_{random_state}_.csv\"\n",
        "#       avg_accuracy = []\n",
        "#       preds = []\n",
        "#       true_labels = []\n",
        "      \n",
        "#       to_save_array = np.zeros((test_dataset[0].shape[0], test_dataset[0].shape[1]), dtype=object)\n",
        "#       test_X_missing = np.empty((test_dataset[0].shape[0] * 2, test_dataset[0].shape[1]), dtype=test_dataset[0].dtype)\n",
        "#       map_values_1_vec = np.vectorize(map_values_1)\n",
        "#       map_values_2_vec = np.vectorize(map_values_2)\n",
        "#       test_X_missing[0::2] = map_values_1_vec(test_dataset[0])\n",
        "#       test_X_missing[1::2] = map_values_2_vec(test_dataset[0])\n",
        "#       test_X_missing = to_categorical(test_X_missing, 3)\n",
        "#       for i in tqdm(range(test_dataset[0].shape[0])):\n",
        "#           rng = np.random.default_rng(i)\n",
        "#           # Generates missing genotypes\n",
        "#           missing_size = int(missing_perc * test_X_missing.shape[1])\n",
        "#           missing_index = rng.integers(low=0, high=test_X_missing.shape[1],\n",
        "#                                             size=missing_size)\n",
        "#           test_X_missing[i*2:i*2+2, missing_index, :] = [0, 0, 1]\n",
        "#           # mMask = np.tile(attention_mask, (1, 1, 1))\n",
        "#           # predict\n",
        "#           predict_onehot = model.predict(test_X_missing[i*2:i*2+2, :, :])\n",
        "#           # predict_onehot = model.predict([test_X_missing[i:i + 1, :, :], mMask])\n",
        "#           # only care the missing position\n",
        "#           predict_missing_onehot = predict_onehot[0:2, missing_index, :]\n",
        "#           # predict label\n",
        "#           predict_missing = np.argmax(predict_missing_onehot, axis=2)\n",
        "#           predict_missing_final = np.zeros((1, predict_missing.shape[1]))\n",
        "#           for j in range(predict_missing.shape[1]):\n",
        "#             if predict_missing[:, j].tolist() == [0, 0]:\n",
        "#               predict_missing_final[:, j] = 0\n",
        "#             elif predict_missing[:, j].tolist() == [0, 1]:\n",
        "#               predict_missing_final[:, j] = 1\n",
        "#             elif predict_missing[:, j].tolist() == [1, 0]:\n",
        "#               predict_missing_final[:, j] = 2\n",
        "#             elif predict_missing[:, j].tolist() == [1, 1]:\n",
        "#               predict_missing_final[:, j] = 3\n",
        "#             else:\n",
        "#               predict_missing_final[:, j] = 4\n",
        "#           preds.extend(predict_missing_final.ravel().tolist())\n",
        "          \n",
        "#           predict_haplotypes = np.argmax(predict_onehot, axis=2)\n",
        "#           for j in range(predict_onehot.shape[1]):\n",
        "#             if predict_haplotypes[:, j].tolist() == [0,0]:\n",
        "#               to_save_array[i, j] = '0|0'\n",
        "#             elif predict_haplotypes[:, j].tolist() == [0,1]:\n",
        "#               to_save_array[i, j] = '0|1'\n",
        "#             elif predict_haplotypes[:, j].tolist() == [1,0]:\n",
        "#               to_save_array[i, j] = '1|0'\n",
        "#             elif predict_haplotypes[:, j].tolist() == [1, 1]:\n",
        "#               to_save_array[i, j] = '1|1'\n",
        "#             else:\n",
        "#               to_save_array[i, j] = '.|.'\n",
        "#           # real label\n",
        "#           # label_missing_onehot = test_dataset[0][i:i + 1, missing_index, :]\n",
        "#           # label_missing = np.argmax(label_missing_onehot, axis=2)\n",
        "#           label_missing = test_dataset[0][i:i + 1, missing_index]\n",
        "#           true_labels.extend(label_missing.ravel().tolist())\n",
        "#           # accuracy\n",
        "#           correct_prediction = np.equal(predict_missing_final, label_missing)\n",
        "#           accuracy = np.mean(correct_prediction)\n",
        "#           # print('{}/{}, accuracy: {:.4f}'.format(\n",
        "#           #     i, test_X_missing.shape[0], accuracy))\n",
        "\n",
        "#           avg_accuracy.append(accuracy)\n",
        "\n",
        "#       # df = pd.DataFrame(to_save_array, columns= headers[:-1], index = test_indices)\n",
        "#       # df.to_csv(save_name)\n",
        "#       print('The average imputation accuracy' \\\n",
        "#             'on test data with {} missing genotypes is {:.4f}: '\n",
        "#           .format(missing_perc, np.mean(avg_accuracy)))\n",
        "#       cnf_matrix = confusion_matrix(true_labels, preds)\n",
        "#       FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
        "#       FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
        "#       TP = np.diag(cnf_matrix)\n",
        "#       TN = cnf_matrix.sum() - (FP + FN + TP)\n",
        "#       FP = FP.astype(float)\n",
        "#       FN = FN.astype(float)\n",
        "#       TP = TP.astype(float)\n",
        "#       TN = TN.astype(float)\n",
        "#       # Sensitivity, hit rate, recall, or true positive rate\n",
        "#       TPR = TP/(TP+FN)\n",
        "#       # Specificity or true negative rate\n",
        "#       TNR = TN/(TN+FP)\n",
        "#       print(f\"Sensitivity: {np.mean(TPR)}\")\n",
        "#       print(f\"Specificity: {np.mean(TNR)}\")\n",
        "#       print(f\"F1-score macro: {f1_score(true_labels, preds, average='macro')}\")\n",
        "#       print(f\"F1-score micro: {f1_score(true_labels, preds, average='micro')}\")\n",
        "# #     accuracies.append(np.mean(avg_accuracy))\n",
        "# #     print(f\"=====================================\")\n",
        "# # print('--------------------------------------------------------------------------')\n",
        "# # print('--------------------------------------------------------------------------')\n",
        "# # print('The overal average imputation accuracy over 10 runs' \\\n",
        "# #         'on test data with {} missing genotypes is {:.4f}: '\n",
        "# #       .format(missing_perc, np.mean(accuracies)))\n",
        "# # print('--------------------------------------------------------------------------')\n",
        "# # print(accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojEbtYYKqrL9",
        "outputId": "e952944d-fe28-4c85-8481-ceabe43564a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbe3d0eeb90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbe3d0eeb90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average imputation accuracyon test data with 0.1 missing genotypes is 0.9968: \n",
            "Sensitivity: 0.9959543456595749\n",
            "Specificity: 0.9988387118258647\n",
            "F1-score macro: 0.9958682410158115\n",
            "F1-score micro: 0.9968480236256247\n"
          ]
        }
      ],
      "source": [
        "# for missing_perc in [\n",
        "#                     #  0.01, 0.05,\n",
        "#                      0.1,\n",
        "#                     #  0.2\n",
        "#                      ]:\n",
        "#   # save_name = f\"./drive/My Drive/ShiLab/HLA/mixed_pop/preds_mixed_mr_{missing_perc}_rs_{random_state}_.csv\"\n",
        "#   avg_accuracy = []\n",
        "#   preds = []\n",
        "#   true_labels = []\n",
        "  \n",
        "#   to_save_array = np.zeros((test_dataset[0].shape[0], test_dataset[0].shape[1]), dtype=object)\n",
        "#   test_X_missing = np.empty((test_dataset[0].shape[0] * 2, test_dataset[0].shape[1]), dtype=test_dataset[0].dtype)\n",
        "#   map_values_1_vec = np.vectorize(map_values_1)\n",
        "#   map_values_2_vec = np.vectorize(map_values_2)\n",
        "#   test_X_missing[0::2] = map_values_1_vec(test_dataset[0])\n",
        "#   test_X_missing[1::2] = map_values_2_vec(test_dataset[0])\n",
        "#   test_X_missing = to_categorical(test_X_missing, 3)\n",
        "#   for i in tqdm(range(test_dataset[0].shape[0])):\n",
        "#       rng = np.random.default_rng(i)\n",
        "#       # Generates missing genotypes\n",
        "#       missing_size = int(missing_perc * test_dataset[0].shape[1])\n",
        "#       missing_index = rng.integers(low=0, high=test_dataset[0].shape[1],\n",
        "#                                         size=missing_size)\n",
        "#       test_X_missing[i*2:i*2+2, missing_index, :] = [0, 0, 1]\n",
        "#       # mMask = np.tile(attention_mask, (1, 1, 1))\n",
        "#       # predict\n",
        "#       predict_onehot = model.predict(test_X_missing[i*2:i*2+2, :, :])\n",
        "#       # predict_onehot = model.predict([test_X_missing[i:i + 1, :, :], mMask])\n",
        "#       # only care the missing position\n",
        "#       predict_missing_onehot = predict_onehot[0:2, list(filter(lambda x: x<chunk_size, missing_index)), :]\n",
        "#       # predict label\n",
        "#       predict_missing = np.argmax(predict_missing_onehot, axis=2)\n",
        "#       predict_missing_final = np.zeros((1, predict_missing.shape[1]))\n",
        "#       for j in range(predict_missing.shape[1]):\n",
        "#         if predict_missing[:, j].tolist() == [0, 0]:\n",
        "#           predict_missing_final[:, j] = 0\n",
        "#         elif predict_missing[:, j].tolist() == [0, 1]:\n",
        "#           predict_missing_final[:, j] = 1\n",
        "#         elif predict_missing[:, j].tolist() == [1, 0]:\n",
        "#           predict_missing_final[:, j] = 2\n",
        "#         elif predict_missing[:, j].tolist() == [1, 1]:\n",
        "#           predict_missing_final[:, j] = 3\n",
        "#         else:\n",
        "#           predict_missing_final[:, j] = 4\n",
        "#       preds.extend(predict_missing_final.ravel().tolist())\n",
        "      \n",
        "#       predict_haplotypes = np.argmax(predict_onehot, axis=2)\n",
        "#       for j in range(predict_onehot.shape[1]):\n",
        "#         if predict_haplotypes[:, j].tolist() == [0,0]:\n",
        "#           to_save_array[i, j] = '0|0'\n",
        "#         elif predict_haplotypes[:, j].tolist() == [0,1]:\n",
        "#           to_save_array[i, j] = '0|1'\n",
        "#         elif predict_haplotypes[:, j].tolist() == [1,0]:\n",
        "#           to_save_array[i, j] = '1|0'\n",
        "#         elif predict_haplotypes[:, j].tolist() == [1, 1]:\n",
        "#           to_save_array[i, j] = '1|1'\n",
        "#         else:\n",
        "#           to_save_array[i, j] = '.|.'\n",
        "#       # real label\n",
        "#       # label_missing_onehot = test_dataset[0][i:i + 1, missing_index, :]\n",
        "#       # label_missing = np.argmax(label_missing_onehot, axis=2)\n",
        "#       label_missing = test_dataset[0][i:i + 1, list(filter(lambda x: x<chunk_size, missing_index))]\n",
        "#       true_labels.extend(label_missing.ravel().tolist())\n",
        "#       # accuracy\n",
        "#       correct_prediction = np.equal(predict_missing_final, label_missing)\n",
        "#       accuracy = np.mean(correct_prediction)\n",
        "#       # print('{}/{}, accuracy: {:.4f}'.format(\n",
        "#       #     i, test_X_missing.shape[0], accuracy))\n",
        "\n",
        "#       avg_accuracy.append(accuracy)\n",
        "\n",
        "#   # df = pd.DataFrame(to_save_array, columns= headers[:-1], index = test_indices)\n",
        "#   # df.to_csv(save_name)\n",
        "#   print('The average imputation accuracy' \\\n",
        "#         'on test data with {} missing genotypes is {:.4f}: '\n",
        "#       .format(missing_perc, np.mean(avg_accuracy)))\n",
        "#   cnf_matrix = confusion_matrix(true_labels, preds)\n",
        "#   FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
        "#   FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
        "#   TP = np.diag(cnf_matrix)\n",
        "#   TN = cnf_matrix.sum() - (FP + FN + TP)\n",
        "#   FP = FP.astype(float)\n",
        "#   FN = FN.astype(float)\n",
        "#   TP = TP.astype(float)\n",
        "#   TN = TN.astype(float)\n",
        "#   # Sensitivity, hit rate, recall, or true positive rate\n",
        "#   TPR = TP/(TP+FN)\n",
        "#   # Specificity or true negative rate\n",
        "#   TNR = TN/(TN+FP)\n",
        "#   print(f\"Sensitivity: {np.mean(TPR)}\")\n",
        "#   print(f\"Specificity: {np.mean(TNR)}\")\n",
        "#   print(f\"F1-score macro: {f1_score(true_labels, preds, average='macro')}\")\n",
        "#   print(f\"F1-score micro: {f1_score(true_labels, preds, average='micro')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOQ-NFQfRgY8"
      },
      "outputs": [],
      "source": [
        "# class CrossAttentionLayer(layers.Layer):\n",
        "#   def __init__(self, local_dim, global_dim, activation=tf.nn.gelu, dropout_rate=0.1,):\n",
        "#     super(CrossAttentionLayer, self).__init__()\n",
        "#     self.local_dim = local_dim\n",
        "#     self.global_dim = global_dim\n",
        "#     self.dropout_rate = dropout_rate\n",
        "#     self.activation = activation\n",
        "#     self.layer_norm00 = layers.LayerNormalization()\n",
        "#     self.layer_norm01 = layers.LayerNormalization()\n",
        "#     self.layer_norm1 = layers.LayerNormalization()\n",
        "#     self.ffn = tf.keras.Sequential(\n",
        "#           [\n",
        "#             layers.Dense(self.local_dim//2, activation=self.activation, \n",
        "#                         ),\n",
        "#             layers.Dense(self.local_dim, \n",
        "#                         activation=self.activation,\n",
        "#                         ), ]\n",
        "#       )\n",
        "#     self.query_encoder = layers.Dense(units=self.local_dim)\n",
        "#     self.key_encoder = layers.Dense(units=self.global_dim)\n",
        "#     self.value_encoder = layers.Dense(units=self.global_dim)\n",
        "#     self.add0 = layers.Add()\n",
        "#     self.add1 = layers.Add()\n",
        "#     self.attention = layers.Attention(use_scale=True, dropout=self.dropout_rate)\n",
        "\n",
        "#   def call(self, inputs, training):\n",
        "#     local_repr = self.layer_norm00(inputs[0])\n",
        "#     global_repr = self.layer_norm01(inputs[1])\n",
        "#     # Create query tensor: [1, latent_dim, projection_dim].\n",
        "#     query = self.query_encoder(local_repr)\n",
        "#     # Create key tensor: [batch_size, data_dim, projection_dim].\n",
        "#     key = self.key_encoder(global_repr)\n",
        "#     # Create value tensor: [batch_size, data_dim, projection_dim].\n",
        "#     value = self.value_encoder(global_repr)\n",
        "\n",
        "#     # Generate cross-attention outputs: [batch_size, latent_dim, projection_dim].\n",
        "#     attention_output = self.attention(\n",
        "#         [query, key, value], return_attention_scores=False\n",
        "#     )\n",
        "#     # Skip connection 1.\n",
        "#     attention_output = self.add0([attention_output, query])\n",
        "\n",
        "#     # Apply layer norm.\n",
        "#     attention_output = self.layer_norm1(attention_output)\n",
        "#     # Apply Feedforward network.\n",
        "#     outputs = self.ffn(attention_output)\n",
        "#     # Skip connection 2.\n",
        "#     outputs = self.add1([outputs, attention_output])\n",
        "#     return outputs\n",
        "\n",
        "# class MaskedTransformerBlock(layers.Layer):\n",
        "#   def __init__(self, embed_dim, num_heads, ff_dim, attention_range, start_offset=0, end_offset=0, activation=tf.nn.gelu, dropout_rate=0.1, use_ffn=True):\n",
        "#     super(MaskedTransformerBlock, self).__init__()\n",
        "#     self.embed_dim = embed_dim\n",
        "#     self.num_heads = num_heads\n",
        "#     self.ff_dim = ff_dim\n",
        "#     self.start_offset = start_offset\n",
        "#     self.end_offset = end_offset\n",
        "#     self.attention_range = attention_range\n",
        "#     self.activation = activation\n",
        "#     self.dropout_rate = dropout_rate\n",
        "#     self.use_ffn = use_ffn\n",
        "#     self.att0 = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_dim, dropout=self.dropout_rate)\n",
        "#     if self.use_ffn:\n",
        "#       self.ffn = tf.keras.Sequential(\n",
        "#           [\n",
        "#             layers.Dense(self.ff_dim, activation=self.activation, \n",
        "#                         ),\n",
        "#             layers.Dense(self.embed_dim, \n",
        "#                         activation=self.activation,\n",
        "#                         ), ]\n",
        "#       )\n",
        "#     self.layer_norm0 = layers.LayerNormalization()\n",
        "#     self.layer_norm1 = layers.LayerNormalization()\n",
        "\n",
        "#   def build(self, input_shape):\n",
        "#     assert(self.end_offset >= 0)\n",
        "#     self.feature_size = input_shape[1]\n",
        "#     attention_mask = np.zeros((self.feature_size,\n",
        "#                                self.feature_size), dtype=bool)\n",
        "#     for i in range(self.start_offset, self.feature_size - self.end_offset):\n",
        "#       attention_indices = np.arange(max(0, i-self.attention_range), min(self.feature_size, i+self.attention_range))\n",
        "#       attention_mask[i, attention_indices] = True\n",
        "    \n",
        "#     # np.repeat(a.reshape((1, a.shape[0], a.shape[1])), 3, axis=0)\n",
        "#     # self.attention_mask = attention_mask[self.start_offset:self.feature_size-self.end_offset]\n",
        "#     # self.attention_mask = self.attention_mask.reshape((1, self.attention_mask.shape[0], self.attention_mask.shape[1]))\n",
        "#     self.attention_mask = tf.constant(attention_mask[self.start_offset:self.feature_size-self.end_offset])\n",
        "\n",
        "#   def call(self, inputs, training):\n",
        "#     # # x = self.concat([inputs[0], inputs[1]])\n",
        "#     # if type(inputs.shape[0]) is int:\n",
        "#     #   attn_mask = tf.constant(np.repeat(self.attention_mask, inputs.shape[0], axis=0))\n",
        "#     # else:\n",
        "#     #   attn_mask = tf.constant(self.attention_mask.reshape((self.attention_mask.shape[1], self.attention_mask.shape[2])))\n",
        "#     x = self.layer_norm0(inputs)\n",
        "#     # x = inputs\n",
        "#     # attn_output = self.att0(x, x)\n",
        "#     attn_output = self.att0(x[:, self.start_offset:x.shape[1]-self.end_offset, :], x,\n",
        "#                             # attention_mask=tf.expand_dims(self.attention_mask, 0)\n",
        "#                             )\n",
        "#     # out1 = x + attn_output\n",
        "#     out1 = x[:, self.start_offset:x.shape[1]-self.end_offset, :] + attn_output\n",
        "#     out1 = self.layer_norm1(out1)\n",
        "#     if self.use_ffn:\n",
        "#       ffn_output = self.ffn(out1)\n",
        "#       return out1 + ffn_output\n",
        "#     else:\n",
        "#       return out1\n",
        "\n",
        "# @tf.function\n",
        "# def my_mat_mul(embedding, input):\n",
        "#   y = tf.einsum('ijk,jkl->ijl', input, embedding)\n",
        "#   return y\n",
        "\n",
        "# class GenoEmbeddings(layers.Layer):\n",
        "#   def __init__(self, embedding_dim, \n",
        "#                embeddings_initializer='glorot_uniform',\n",
        "#                embeddings_regularizer=None,\n",
        "#                activity_regularizer=None,\n",
        "#                embeddings_constraint=None):\n",
        "#     super(GenoEmbeddings, self).__init__()\n",
        "#     self.embedding_dim = embedding_dim\n",
        "#     self.embeddings_initializer = initializers.get(embeddings_initializer)\n",
        "#     self.embeddings_regularizer = regularizers.get(embeddings_regularizer)\n",
        "#     self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "#     self.embeddings_constraint = constraints.get(embeddings_constraint)\n",
        "\n",
        "  \n",
        "#   def get_config(self):\n",
        "#     config = super().get_config().copy()\n",
        "#     config.update({\n",
        "#         'embedding_dim': self.embedding_dim,\n",
        "#         'embeddings_initializer': self.embeddings_initializer,\n",
        "#         'embeddings_regularizer': self.embeddings_regularizer,\n",
        "#         'activity_regularizer': self.activity_regularizer,\n",
        "#         'embeddings_constraint': self.embeddings_constraint,\n",
        "#         # 'att0': self.att0,\n",
        "#         # 'ffn': self.ffn,\n",
        "#         # 'dropout1': self.dropout1,\n",
        "#         # 'dropout2': self.dropout2\n",
        "#     })\n",
        "#     return config\n",
        "\n",
        "#   def build(self, input_shape):\n",
        "#     # print(input_shape)\n",
        "#     self.num_of_allels = input_shape[-1]\n",
        "#     self.n_snps = input_shape[-2]\n",
        "#     self.embedding = self.add_weight(\n",
        "#             shape=(self.n_snps, self.num_of_allels, self.embedding_dim),\n",
        "#             initializer=self.embeddings_initializer,\n",
        "#             trainable=True, name='geno_embeddings',\n",
        "#             regularizer=self.embeddings_regularizer,\n",
        "#             constraint=self.embeddings_constraint,\n",
        "#             experimental_autocast=False\n",
        "#         )\n",
        "#     # self.position_embedding = layers.Embedding(\n",
        "#     #         input_dim=self.n_snps, output_dim=self.embedding_dim\n",
        "#     #     )\n",
        "#   def call(self, inputs):\n",
        "#     # positions = tf.range(start=0, limit=self.n_snps, delta=1)\n",
        "#     return my_mat_mul(self.embedding, inputs)# + self.position_embedding(positions)\n",
        "\n",
        "# class Chunker(layers.Layer):\n",
        "#   def __init__(self, embed_dim, num_heads, ff_dim, chk_size=chunk_size,\n",
        "#                activation=tf.nn.gelu, dropout_rate=0.25,\n",
        "#                attention_range=attention_range, include_embedding_layer=False):\n",
        "#     super(Chunker, self).__init__()\n",
        "#     self.concat = layers.Concatenate(axis=-2)\n",
        "#     self.chunk_size = chk_size\n",
        "#     self.embed_dim = embed_dim\n",
        "#     self.num_heads = num_heads\n",
        "#     self.ff_dim = ff_dim\n",
        "#     self.activation = activation\n",
        "#     self.dropout_rate = dropout_rate\n",
        "#     self.attention_range = attention_range\n",
        "#     self.include_embedding_layer = include_embedding_layer\n",
        "#     # self.ln = layers.LayerNormalization()\n",
        "\n",
        "#     # self.num_conv_layers = 2\n",
        "#     # self.num_output_channels = [embed_dim//2, embed_dim]\n",
        "#     # self.kernel_size = 3\n",
        "#     # self.stride = 1\n",
        "#     # self.padding = 1\n",
        "#     # self.pooling_kernel_size = 3\n",
        "#     # self.pooling_stride = 2\n",
        "\n",
        "#     # self.conv_model = keras.Sequential()\n",
        "#     # for i in range(self.num_conv_layers):\n",
        "#     #   self.conv_model.add(\n",
        "#     #       layers.Conv1D(\n",
        "#     #           self.num_output_channels[i],\n",
        "#     #           self.kernel_size,\n",
        "#     #           self.stride,\n",
        "#     #           padding=\"valid\",\n",
        "#     #           use_bias=False,\n",
        "#     #           activation=tf.nn.gelu,\n",
        "#     #           kernel_initializer=\"he_normal\",\n",
        "#     #       )\n",
        "#     #   )\n",
        "#     #   # self.conv_model.add(layers.ZeroPadding1D(self.padding))\n",
        "#     #   self.conv_model.add(\n",
        "#     #       layers.MaxPool1D(self.pooling_kernel_size, self.pooling_stride, \"same\")\n",
        "#     #   )\n",
        "#     # # self.conv_model.add(layers.BatchNormalization())\n",
        "#     # # self.conv_model.add(layers.Dropout(self.dropout_rate))\n",
        "    \n",
        "#   def build(self, input_shape):\n",
        "#     self.chunk_starts = list(range(0, input_shape[1], self.chunk_size))\n",
        "#     self.chunk_ends = []\n",
        "#     for cs in self.chunk_starts:\n",
        "#       self.chunk_ends.append(min(cs+self.chunk_size, input_shape[1]))\n",
        "#     self.mask_starts = [max(0, cs-self.attention_range) for cs in self.chunk_starts]\n",
        "#     self.mask_ends = [min(ce+self.attention_range, input_shape[1]) for ce in self.chunk_ends]\n",
        "#     self.chunkers = [Chunk(self.embed_dim, self.num_heads, self.ff_dim,\n",
        "#                            attention_range,\n",
        "#                            include_embedding_layer=self.include_embedding_layer,\n",
        "#                            start_offset=cs - self.mask_starts[i],\n",
        "#                             end_offset=self.mask_ends[i]-self.chunk_ends[i]) for i, cs in enumerate(self.chunk_starts)]\n",
        "#     # self.cross_attentions = [CrossAttentionLayer(self.embed_dim, self.embed_dim) for i, cs in enumerate(self.chunk_starts)]\n",
        "\n",
        "#   def call(self, inputs, training):\n",
        "#     x = inputs\n",
        "#     # compact_repr = self.conv_model(x)\n",
        "#     # print(compact_repr.shape)\n",
        "#     chunks = [chunker(x[:, self.mask_starts[i]:self.mask_ends[i]]) for i, chunker in enumerate(self.chunkers)]\n",
        "#     y = self.concat(chunks)\n",
        "#     # compact_repr = self.conv_model(y)\n",
        "#     # chunks = [self.cross_attentions[i]([y[:, self.chunk_starts[i]:self.chunk_ends[i]], compact_repr]) for i, chunker in enumerate(self.chunkers)]\n",
        "#     # chunks = [chunker(x[:, self.mask_starts[i]:self.mask_ends[i]]) + \n",
        "#     #           self.cross_attentions[i]([x[:, self.chunk_starts[i]:self.chunk_ends[i]], compact_repr]) for i, chunker in enumerate(self.chunkers)]\n",
        "#     # chunks_reloaded = [cross_attention([x[:, self.chunk_starts[i]:self.chunk_ends[i]], compact_repr]) for i, cross_attention in enumerate(self.cross_attentions)]\n",
        "#     # y = [chunks[i] + chunks_reloaded[i] for i in range(len(self.chunk_starts))]\n",
        "#     # y = self.concat(chunks)\n",
        "#     # y = self.ln(y)\n",
        "#     return y\n",
        "\n",
        "\n",
        "# class Chunk(layers.Layer):\n",
        "#   def __init__(self, embed_dim, num_heads, ff_dim, attention_range, start_offset=0, end_offset=0, include_embedding_layer=False):\n",
        "#     super(Chunk, self).__init__()\n",
        "#     self.attention_range = attention_range\n",
        "#     self.ff_dim = ff_dim\n",
        "#     self.num_heads = num_heads\n",
        "#     self.embed_dim = embed_dim\n",
        "#     self.include_embedding_layer = include_embedding_layer\n",
        "\n",
        "#     self.attention_block = MaskedTransformerBlock(self.embed_dim,\n",
        "#                                                    self.num_heads, self.ff_dim,\n",
        "#                                                    attention_range, start_offset,\n",
        "#                                                    end_offset)\n",
        "#     if include_embedding_layer:\n",
        "#       self.embedding = GenoEmbeddings(embed_dim)\n",
        "    \n",
        "\n",
        "#   def build(self, input_shape):\n",
        "#     # print(input_shape, input_shape[1])\n",
        "#     # self.latent_array = self.add_weight(\n",
        "#     #         shape=(input_shape[1] - self.end_offset - self.start_offset, self.embed_dim),\n",
        "#     #         initializer=\"random_normal\",\n",
        "#     #         trainable=True,\n",
        "#     #     )\n",
        "#     # Create cross-attenion module.\n",
        "#     # self.cross_attention = create_cross_attention_module(\n",
        "#     #     input_shape[1] - self.end_offset - self.start_offset,\n",
        "#     #     input_shape[1],\n",
        "#     #     self.embed_dim,\n",
        "#     #     self.embed_dim // 2,\n",
        "#     #     self.dropout_rate,\n",
        "#     # )\n",
        "#     pass\n",
        "  \n",
        "#   def call(self, inputs, training):\n",
        "#     if self.include_embedding_layer:\n",
        "#       x = self.embedding(inputs)\n",
        "#     else:\n",
        "#       x = inputs\n",
        "#     x = self.attention_block(x)\n",
        "#     return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YljwvovBGha"
      },
      "outputs": [],
      "source": [
        "# def create_model():\n",
        "#   embed_dim = 64\n",
        "#   inputt = layers.Input(shape=(feature_size, inChannel))\n",
        "  \n",
        "#   xa = GenoEmbeddings(embed_dim)(inputt)\n",
        "#   # xa = Chunker(embed_dim, num_heads, embed_dim//2, include_embedding_layer=True)(inputt)\n",
        "#   # xa = MaskedTransformerBlock(embed_dim, num_heads, embed_dim//2, attention_range)(inputt)\n",
        "#   xa = layers.BatchNormalization()(xa)\n",
        "#   xa0 = layers.Dense(embed_dim, activation=tf.nn.gelu)(xa)\n",
        "#   # xa0 = layers.Dense(embed_dim//2, activation=tf.nn.gelu)(xa)\n",
        "#   # xa0 = layers.Dense(embed_dim//2, activation=tf.nn.gelu)(xa0)\n",
        "#   # xa0 = layers.BatchNormalization(epsilon=2e-5, momentum=9e-1)(xa0)\n",
        "#   xa = layers.Conv1D(embed_dim, 3, padding='same', activation=tf.nn.gelu,\n",
        "#                     # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#                     )(xa)\n",
        "#   xb0 = layers.Conv1D(embed_dim, 7, padding='same', activation=tf.nn.gelu,\n",
        "#                     # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#                     )(xa)\n",
        "#   xb0 = layers.Conv1D(embed_dim, 15, padding='same', activation=tf.nn.gelu,\n",
        "#                     # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#                     )(xb0)\n",
        "  \n",
        "\n",
        "#   xb1 = layers.Conv1D(embed_dim, 5, padding='same', activation=tf.nn.gelu,\n",
        "#                     # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#                     )(xa)\n",
        "#   xb1 = layers.Conv1D(embed_dim, 7, padding='same', activation=tf.nn.gelu,\n",
        "#                     # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#                     )(xb1)\n",
        "\n",
        "\n",
        "#   xb = layers.Add()([xa0, xb0, xb1])\n",
        "#   xb = layers.Conv1D(embed_dim, 3, padding='same', activation=tf.nn.gelu,)(xb)\n",
        "#   xb = layers.BatchNormalization()(xb)\n",
        "#   xb = layers.DepthwiseConv1D(embed_dim, 1, padding='same')(xb)\n",
        "#   xb = layers.BatchNormalization()(xb)\n",
        "#   xa = layers.Activation(tf.nn.gelu)(xb)\n",
        "\n",
        "\n",
        "#   # xa = layers.MaxPooling1D(2)(xa)\n",
        "\n",
        "\n",
        "#   # embed_dim = int(embed_dim*2)\n",
        "#   # # xa = layers.Dense(embed_dim//2, activation=tf.nn.gelu)(xa)\n",
        "#   # # xa0 = layers.Dense(embed_dim//2, activation=tf.nn.gelu)(xa)\n",
        "#   # # xa0 = layers.BatchNormalization(epsilon=2e-5, momentum=9e-1)(xa0)\n",
        "#   # xa0 = layers.Dense(embed_dim, activation=tf.nn.gelu)(xa)\n",
        "#   # xa = layers.Conv1D(embed_dim, 3, padding='same', activation=tf.nn.gelu,\n",
        "#   #                   # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#   #                   )(xa)\n",
        "#   # xb0 = layers.Conv1D(embed_dim, 7, padding='same', activation=tf.nn.gelu,\n",
        "#   #                   # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#   #                   )(xa)\n",
        "#   # xb0 = layers.Conv1D(embed_dim, 15, padding='same', activation=tf.nn.gelu,\n",
        "#   #                   # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#   #                   )(xb0)\n",
        "  \n",
        "\n",
        "#   # xb1 = layers.Conv1D(embed_dim, 5, padding='same', activation=tf.nn.gelu,\n",
        "#   #                   # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#   #                   )(xa)\n",
        "#   # xb1 = layers.Conv1D(embed_dim, 7, padding='same', activation=tf.nn.gelu,\n",
        "#   #                   # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#   #                   )(xb1)\n",
        "\n",
        "#   # xb = layers.Add()([xa0, xb0, xb1])\n",
        "#   # xb = layers.Conv1D(embed_dim, 3, padding='same', activation=tf.nn.gelu,)(xb)\n",
        "#   # xb = layers.BatchNormalization()(xb)\n",
        "#   # xb = layers.DepthwiseConv1D(embed_dim, 1, padding='same')(xb)\n",
        "#   # xb = layers.BatchNormalization()(xb)\n",
        "#   # xa = layers.Activation(tf.nn.gelu)(xb)\n",
        "  \n",
        "#   # xa = layers.UpSampling1D(2)(xa)\n",
        "#   # embed_dim = int(embed_dim/2)\n",
        "#   # xa = layers.Dense(embed_dim//2, activation=tf.nn.gelu)(xa)\n",
        "#   # xa0 = layers.Dense(embed_dim//2, activation=tf.nn.gelu)(xa)\n",
        "#   # xa0 = layers.BatchNormalization(epsilon=2e-5, momentum=9e-1)(xa0)\n",
        "#   xa0 = layers.Dense(embed_dim//2, activation=tf.nn.gelu)(xa)\n",
        "#   xa = layers.Conv1D(embed_dim//2, 3, padding='same', activation=tf.nn.gelu,\n",
        "#                     # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#                     )(xa)\n",
        "#   xb0 = layers.Conv1D(embed_dim//2, 7, padding='same', activation=tf.nn.gelu,\n",
        "#                     # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#                     )(xa)\n",
        "#   xb0 = layers.Conv1D(embed_dim//2, 15, padding='same', activation=tf.nn.gelu,\n",
        "#                     # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#                     )(xb0)\n",
        "  \n",
        "\n",
        "#   xb1 = layers.Conv1D(embed_dim//2, 5, padding='same', activation=tf.nn.gelu,\n",
        "#                     # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#                     )(xa)\n",
        "#   xb1 = layers.Conv1D(embed_dim//2, 7, padding='same', activation=tf.nn.gelu,\n",
        "#                     # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#                     )(xb1)\n",
        "\n",
        "#   xb = layers.Add()([xa0, xb0, xb1])\n",
        "#   xb = layers.Conv1D(embed_dim//2, 3, padding='same', activation=tf.nn.gelu,)(xb)\n",
        "#   xb = layers.BatchNormalization()(xb)\n",
        "#   xb = layers.DepthwiseConv1D(embed_dim//2, 1, padding='same')(xb)\n",
        "#   xb = layers.BatchNormalization()(xb)\n",
        "#   xa = layers.Activation(tf.nn.gelu)(xb)\n",
        "\n",
        "\n",
        "#   # xa = layers.UpSampling1D(2)(xa)\n",
        "#   # embed_dim = int(embed_dim/2)\n",
        "#   # # xa = layers.Dense(embed_dim//2, activation=tf.nn.gelu)(xa)\n",
        "#   # # xa0 = layers.Dense(embed_dim//2, activation=tf.nn.gelu)(xa)\n",
        "#   # # xa0 = layers.BatchNormalization(epsilon=2e-5, momentum=9e-1)(xa0)\n",
        "#   # xa0 = layers.Dense(embed_dim//2, activation=tf.nn.gelu)(xa)\n",
        "#   # xa = layers.Conv1D(embed_dim//2, 3, padding='same', activation=tf.nn.gelu,\n",
        "#   #                   # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#   #                   )(xa)\n",
        "#   # xb0 = layers.Conv1D(embed_dim//2, 7, padding='same', activation=tf.nn.gelu,\n",
        "#   #                   # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#   #                   )(xa)\n",
        "#   # xb0 = layers.Conv1D(embed_dim//2, 15, padding='same', activation=tf.nn.gelu,\n",
        "#   #                   # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#   #                   )(xb0)\n",
        "  \n",
        "\n",
        "#   # xb1 = layers.Conv1D(embed_dim//2, 5, padding='same', activation=tf.nn.gelu,\n",
        "#   #                   # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#   #                   )(xa)\n",
        "#   # xb1 = layers.Conv1D(embed_dim//2, 7, padding='same', activation=tf.nn.gelu,\n",
        "#   #                   # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#   #                   )(xb1)\n",
        "\n",
        "#   # xb = layers.Add()([xa0, xb0, xb1])\n",
        "#   # xb = layers.Conv1D(embed_dim//2, 3, padding='same', activation=tf.nn.gelu,)(xb)\n",
        "#   # xb = layers.BatchNormalization()(xb)\n",
        "#   # xb = layers.DepthwiseConv1D(embed_dim//2, 1, padding='same')(xb)\n",
        "#   # xb = layers.BatchNormalization()(xb)\n",
        "#   # xa = layers.Activation(tf.nn.gelu)(xb)\n",
        "  \n",
        "#   embed_dim = int(embed_dim/2)\n",
        "  \n",
        "#   xa = Chunker(embed_dim, num_heads, embed_dim//2)(xa)\n",
        "#   # # for _ in range(4):\n",
        "#   #   # xa = MaskedTransformerBlock(embed_dim, num_heads, embed_dim//2, attention_range)(xa)\n",
        "#   # xa0 = layers.Dense(embed_dim//2, activation=tf.nn.gelu)(xa)\n",
        "#   # # xa0 = layers.Dense(embed_dim//2, activation=tf.nn.gelu)(xa0)\n",
        "#   # # xa0 = layers.BatchNormalization(epsilon=2e-5, momentum=9e-1)(xa0)\n",
        "#   # # xa = Chunker(embed_dim, num_heads, embed_dim//2)(xb)\n",
        "\n",
        "#   # xb = layers.Conv1D(embed_dim//2, 3, padding='same', activation=tf.nn.gelu,\n",
        "#   #                   # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#   #                   )(xa)\n",
        "#   # xb = layers.Conv1D(embed_dim//2, 5, padding='same', activation=tf.nn.gelu,\n",
        "#   #                   # kernel_regularizer=regularizers.l1(regularization_coef_l1),\n",
        "#   #                   )(xb)\n",
        "#   # xb = layers.Add()([xa0, xb])\n",
        "#   # xb = layers.Conv1D(embed_dim//2, 3, padding='same', activation=tf.nn.gelu,)(xb)\n",
        "#   # xb = layers.BatchNormalization(epsilon=2e-5, momentum=9e-1)(xb)\n",
        "  \n",
        "  \n",
        "#   output1 = layers.Conv1D(inChannel, 5, padding='same', activation=tf.nn.softmax)(xa)\n",
        "\n",
        "#   # output1 = layers.Dense(inChannel, activation='softmax')(xa)\n",
        "#   classifier = MaskedGenotypeModel(inputt, output1, name=\"masked_geno_model\")\n",
        "#   # classifier = MaskedGenotypeModel([inputt, masks], output1, name=\"masked_geno_model\")\n",
        "#   # classifier= tf.keras.Model(inputs=[inputt, masks], outputs=output1)\n",
        "#   # classifier= keras.Model(inputs=inputt, outputs=[output0, output1])\n",
        "#   # classifier.summary()\n",
        "\n",
        "#   return classifier"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "[TPU][BEST] SplitTransformer Imputation Masked Attention-HLA-Split Haplotypes VX0.1",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
